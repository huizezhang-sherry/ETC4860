Hi Everyone, Today I will be talking about exploring the judicial facial expression in videos of legal proceedings. 

# Background

There are many videos of legal proceedings that are made available in the High Court of Australia website and here is a fraction of them.

[play video]

As we can see, apart from what the Justices say, there's another source of data that we can get from these videos. They are the facial expressions and we are interested to see how these facial patterns could be used to understand the Justices and further to understand their decision of the cases.

The Justices are expected to behave impartially in the courtroom. However, there are studies showed that speakers themselves can be unaware of some facial and vocal expressions of themselves. 

In 2018,  Tutton has studied the judicial demenaur of the judge using transcript and AV recordings. In their study, the facial expressions of the judge were recorded manually when the videos were played. This observational approach of analysing videos could be  subjective when different people are observing the same videos. This motivates me to study the facial expression of the judge via an more objective approach by using facial recognition technology. 

## Face recognition 
THe facial recognition technology these days are based on Paul Ekman's study of facial action coding system. Action units are fundamental unit of human facial muscles. Here we have two examples of them. Action unit 2 is the raising of outer eyebrow and actio nunit 15 is the lip corner depressor. Dsescribing face using this FACS  has been widely used in human emotion reserach.


## Data 

The source data for this research is the AV recordings publicly available from the High Court of Australia. A sequence of procedure has been employed to obtain the facial related variables of the Justices and they are summarised in the flowchart below. 

After downloading the videos using the command line tool youtub-dl,  we use ffmpeg to chop the video at a rate of one frame per minute, which gives us more than a thousand image frames. 

The next step is to extract all the faces from these images. Because the Justices are remain sitted in the same position through out the hearing, I can locate the x and y coordinates of the judges and use that to crop all the image frames from the same videos. From our one thousand image frames, we finally get  4 thousand and 6 hundred face images. 

These faces are all sent to OpenFace, an open sourced software for facial behaviour analysis. For each of the face image, I get one csv file that contains 711 facial variables. All these csv files are then processed to add metadata. 

Our final data format looks like this. We have judge_id, video_id, frame_id and speakers for each of the action unit. Presence score and intensity score are available for every observation.  


## Method

Before moving into the modelling, let's first define the notation in this project. In our sample, we have six different judges, seven different cases and 18 different action units. Speaker is a binary variable that indicating whether the appellent or the respondent is speaking. There is also a time frame variable t that depends on the length of the video. The y variable is the binary present score, which has value of one if an action unit is present and zero otherwise. Thus, a main effect model without the interactions can be written down as on the screen.

There are a few more steps to do before reaching to our full model. Based on our exploratory data analysis, we want to allow for different action units for the same judge to have different present score, thus we add the interactions between judge and action unit. Based on the same reason, we also add the interactions between judge and video, judge and speaker, case and action unit.

By having this model, we are now able to answer the following two questions. Do the justices' expression differ from case to case and Do the justices' expression differ when different parties are speaking. 

To do this, we compute the estimates of the presence score for each unique combination of judge, video, action unit and speaker, along with the confidence intervals. Because we are computing the confidence intervals for all estimates at once, adjustment need to be made to ensure the overall type I error is less than 0.05. 


## Result 

We can observe that Judge Edelman, Keane and Kiefel behave relatively consistent throughout all the videos since all the intervals for the same judge, same action unit but different videos overlaps after bonferroni adjustment. 

Judge Gageler seems to have a large fluctuate of his facial expressions in video OKS and his response is significantly different from those in other cases for AU15 and AU20. This shows consistency with our exploratory data analysis where Gageler tends to show a higher proportion of presence for action units in case OKS. 

Our result validates the exploratory data analysis that the difference between case OKS and Parkes for Bell are significant in action unit 14, 15 and 20. Since the confidence interval doesn't overlap for these two videos. The difference between case McKell and OKS are not significance after the Bonferroni adjustment and this indicates the difference we see before in Figure \ref{fig:common_video} is more likely due to randomness rather than the true underlying difference between the two videos. From a legal perspective, this would show that Bell is addressing the cases with different responses. However, this different approach of responding by the judge doesn't indicate the biasness of the judge in the courtroom but the individuality of different judge approaching to cases. 


The result for speaker effect shows that the video-wise difference between judge still persist and the speaker-wise difference is not significant. This result would be a validation that on the high court level, the judges are behaving impartial to different speaking parties. 





-----
Legal studies have been tried to predict court outcome from the last century. Early study has been conducted using judge charcteristics for example, gender, religious background and political view. Moving from that, more recent studies in the U.S. have been found to use on-court information to make this prediction. Some of these information include the use of language by judge, the facial expression by judge and voice by judge. 


Rather than focusing on prediction, the work using Australia data is still at the stage to understand the facial expression of the Justices.
