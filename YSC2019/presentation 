Hi Everyone, Today I will be talking about exploring the judicial facial expression in videos of legal proceedings. 

# Background

The videos of legal proceedings are available for cases in the High Court of Australia, and can be found on their website.

Here is a sample from XXX.

[play video]

The Justices are expected to appear impartial in the courtroom. Can you keep a straight face? Its hard not to react emotionally sometimes. 

There have been many studies into facial and vocal expressions of judges, based on court transcripts, or empirical studies. 

In Australia, Tutton, Mark and Roach Anlau published a paper in 2018 detailing their empirical study of transcripts and AV recordings of judges in High Court cases. They found found that the judges behaved with a detached demeanour. Their video analysis was done with manual annotation. 

With the easy availability of facial recognition software it is interesting to automatically collect data on the judges facial expression. This is what I have done, and I have analysed facial expressions of the 6 high court judges in 7 videos of court cases. 

## Face recognition 

The face recognition technology groups movements of facial landmarks into 45 action units. There are many more variables created including where the eyes are looking, and facial landmarks, but we focus only on the action units.

Here is an explanation of two action units.

Action unit 2 is the raising of outer eyebrow. This might be associated with surprise. 

Action unit 15 is the lip corner depressor. This might be associated with disgust.


## Data collection

The flowchart here shows the steps to collecting the data.

The first step to collecting the data is to process the videos. The videos are downloaded from youtube, and chopped into a set of images extracted at 1 minute intervals. This produces 1000 frames. The face recognition is conducted on these still images. 

Faces are extracted from each image. Because the Justices are remain seated in the same position through out the hearing, their faces can be easily extract cropping a fixed region of the image. This yields 4600 images total, and it allows us to separately process data for each judge.

The images are all processed with the face recognition software which tags each face with the facial action unit presence and intensity, along with many other variables like landmarks and eye focus. 

The information is collected into one csv file that contains 711 facial variables. 

One additional step is that the text transcripts were analysed to extract times when the appellant and respondent were addressing the judges. 

## Data format 

The final data looks like this. Columns contain judge_id, video_id, frame_id, speakers (appellant or respondent), action unit, and presence/absence of the action unit, and intensity score for the action unit.  

## Method

The judges facial expressions are modelled using a generalised linear model. 

In the sample, we have 6 different judges, 7 different cases and 18 different action units. 

Speaker is a binary variable that indicating whether the appellant or the respondent is speaking. 

There is also a time variable indicating the 1 minute intervals. 

The response variable used for this model is the binary presence variable, indicating whether the action unit is observed or not. 

alpha_i, beta_j, gamma_k and delta_l represent the effect of judge, video, action unit and speaker, respectively

The interaction term between judge (alpha) and video (beta_j) allows different judges to react differently in different videos. Similarly for judge and action unit, judge and speaker, and action unit and speaker. 


With the model, we are able to answer the following two questions: Do the justices' expression differ from case to case and Do the justices' expression differ when different parties are speaking. 

After the model fit, multiple comparisons are conducted to compare specific effects.


## Result 


This is a plot of the 95% confidence intervals computed by multiple comparison of the means after the model fitting. Bonferroni adjustments were made.

Proportion of frames where the action unit was present is plotted against video. The facets show judge in the columns, and four main action units on the rows. Colour represents video. 

Note that, each case may have different judges. Not all judges sit on each case.

There are so many possible findings to report. Here are the main ones. 

The facial expressions of Judge Edelman, Keane and Kiefel bare relatively consistent throughout all the videos. 

Judge Gageler react differently in the OKS case, on several action units. 

Bell also has more expression on several action units for the OKS case.

-----
Legal studies have been tried to predict court outcome from the last century. Early study has been conducted using judge charcteristics for example, gender, religious background and political view. Moving from that, more recent studies in the U.S. have been found to use on-court information to make this prediction. Some of these information include the use of language by judge, the facial expression by judge and voice by judge. 


Rather than focusing on prediction, the work using Australia data is still at the stage to understand the facial expression of the Justices.




Do the Justices' expression differ

- from one case to another?

- when different parties were speaking? 

Method: 

- See if the confidence interval, after correcting for multiple comparison of the mean overlaps 
