Hi Everyone, Today I will be talking about exploring the judicial facial expression in videos of legal proceedings. You can go to this link to find my slides. 

# Background

The videos of legal proceedings are available from the High Court of Australia, and 

Here is a sample from The republic of Nauru v WET040.

[play video]

The Justices are expected to appear impartial in the courtroom. However, can you keep a straight face? Its hard not to react emotionally sometimes. 

There have been many studies into facial and vocal expressions of judges, based on court transcripts, or empirical studies. 

In Australia, Tutton et al 	published a paper in 2018 detailing their empirical study of transcripts and AV recordings of judges in High Court cases. They found that the judges behaved with a detached demeanour. Their video analysis was done with manual annotation. 

With the easy availability of facial recognition software it is interesting to automatically collect data on the facial expression of the judges. This is what I have done, and I have analysed facial expressions of the 6 high court judges in 7 videos of court cases. 

## Face recognition 

The facial recognition technology is based on the Facial action Coding System of Paul Ekman. 

The face recognition technology groups movements of facial landmarks into 45 action units. There are many more variables created including where the eyes are looking, and facial landmarks, but we focus only on the action units.

Here is an explanation of two action units.

Action unit 2 is the raising of outer eyebrow. This might be associated with surprise. 

Action unit 15 is the lip corner depressor. This might be associated with disgust.


## Data collection

The flowchart I'm going to show you outlines the steps to collecting the data.

The first step to collect the data is to process the videos. The videos are downloaded from the high Court of Australia website, and chopped into a set of images extracted at 1 minute intervals. This produces 1000 frames. The face recognition is conducted on these still images. 

Faces are extracted from each image. Because the Justices are remain seated in the same position through out the hearing, their faces can be easily extracted by cropping a fixed region of the image. This yields 4600 images total. 

The images are all processed with the face recognition software which tags each face with the facial action unit presence and intensity, along with many other variables like landmarks and eye focus. 

The information is collected into one csv file that contains 711 facial variables. 

One additional step is that the text transcripts were analysed to extract times when the appellant and respondent were addressing the judges. 

## Data format 

The final data looks like this. Columns contain judge_id, video_id, frame_id, speakers (appellant or respondent), action unit being analysed, and presence/absence of the action unit, and intensity score for the action unit.  

## Method

In the sample, we have 6 different judges, 7 different cases and 18 different action units. 

Speaker is a binary variable that indicating whether the appellant or the respondent is speaking. 

There is also a time variable indicating the 1 minute intervals. 

The response variable used for this model is the binary presence variable, indicating whether the action unit is observed or not. 

The judges facial expressions are modelled using a generalised linear model. 

alpha_i, beta_j, gamma_k and delta_l represent the effect of judge, video, action unit and speaker, respectively

The interaction term between judge (alpha) and video (beta_j) allows different judges to react differently in different videos. Similarly for judge and action unit, judge and speaker, and action unit and speaker. 


With the model, we are able to answer the following two questions: Do the justices' expression differ from case to case and Do the justices' expression differ when different parties are speaking. 

After the model fit, multiple comparisons are conducted to compare specific effects.


## Result 


This is a plot of the 95% confidence intervals computed by multiple comparison of the means after the model fitting. Bonferroni adjustments were made.

The x axis represents video and y axis represents the proportion of frames in each video where the action unit was present. The facets show judge in the columns, and four main action units on the rows. Colour represents video. 

Note that, each case may have different judges. Not all judges sit on each case.

There are so many possible findings to report. Here are the main ones. 

The facial expressions of Judge Edelman, Keane and Kiefel were relatively consistent throughout all the videos they participated. 

Judge Gageler reacted differently in the OKS case, on the four main action units. 

In the same OKS case, Judge Bell also shows each expression with a higher proportion. 


## Summary of results

Our results mostly validate Tutton et al (2018) that the judges appear impartial, with the exception of Gageler and Bell for the OKS case.

An important emphasis of this work is that "Facial recognition analysis of the videos provides a way to objectively assess judicial behaviour."

## Packages used in the research include

This is the list of software and R packages used for this work.

# Acknowledgement

Thank you for listening
