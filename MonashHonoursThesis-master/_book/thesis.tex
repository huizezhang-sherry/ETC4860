% This is a LaTeX thesis template for Monash University.
% to be used with Rmarkdown
% This template was produced by Rob Hyndman
% Version: 6 September 2016

\documentclass{monashthesis}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Add any LaTeX packages and other preamble here if required
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author{Huize Zhang}
\title{Exploration of Judicial Facial Expression in Videos and Transcripts of Legal Proceedings}
\studentid{27478343}
\def\degreetitle{Bachelor of Commerce (Honours)}
% Add subject and keywords below
\hypersetup{
     %pdfsubject={The Subject},
     %pdfkeywords={Some Keywords},
     pdfauthor={Huize Zhang},
     pdftitle={Exploration of Judicial Facial Expression in Videos and Transcripts of Legal Proceedings},
     pdfproducer={Bookdown with LaTeX}
}


\bibliography{thesisrefs}

\begin{document}

\pagenumbering{roman}

\titlepage

{\setstretch{1.2}\sf\tighttoc\doublespacing}

\clearpage\pagenumbering{arabic}\setcounter{page}{0}

\hypertarget{ch:intro}{%
\chapter{Introduction}\label{ch:intro}}

Decisions by courtroom Justices have been discussed broadly in the legal literature. Gender, political views and religious background of both the Justices and counsel in the case potentially influence the decisions. This paper will explore the facial behaviour of the Justices during hearings with the objective of being to assess whether it can help to predict outcomes. Audio Visual(AV) recordings and case transcripts will be computationally processed and analysed to examine the decisions of each Justice.

\hypertarget{motivation}{%
\section{Motivation}\label{motivation}}

\textcite{tutton2018judicial} attempted to utilize the AV technology, which is made available online by the High Court of Australia \autocite{highcourtau}. They visually inspect of the videos to highlight when Justices depart from the expected norms of judicial conduct. To better understand the emotion status and therefore the departure of the emotional behaviour, more advanced technologies could be applied. An example is to use OpenFace \autocite*{baltrusaitis2018openface} technology, which provides information on emotions exhibited by the Justices. This technique has been applied by \textcite{kovalchik2018going} on professional tennis players during Grand Slam matches. That study demonstrated the potential to predict the outcome of High Court appeals based on Justices' demeanour utilising contemporary tools and emotion tagging techniques.

\hypertarget{literature-review}{%
\section{Literature review}\label{literature-review}}

The literature summary is divided into two parts: (1) current work in legal studies to understand the behaviour of the Justices and (2) existing facial recognition and emotion tagging technology.

\hypertarget{legal-study-from-a-behaviour-perspective}{%
\subsection{Legal study from a behaviour perspective}\label{legal-study-from-a-behaviour-perspective}}

People have attempted to predict the decisions of the Justices for centuries. \textcite{judicalguid} present the following code of conduct:

\begin{quote}
It is important for judges to maintain a standard of behaviour in court that is consistent with the status of judicial office and does not diminish the confidence of litigants in particular, and the public in general, in the ability, the integrity, the impartiality and the independence of the judge.
\end{quote}

This impartiality should be clear in judicial demeanour \autocites{tutton2018judicial}{goffman1956nature}. Paul Ekman \autocite{ekman1991invited} suggests that from a behavioural perspective, some facial and vocal inflections are often unbeknown to the speakers themselves . Many scholars have exploited this in studying the court outcomes through the language and words used by the Justices in the court \autocite{Shullman2004illusion} and vocal and facial characteristics of the Justices \autocite{chen2018justice}.

There are also existing works to understand the emotion of the Justices from a linguistic perspective and suggest some factors that could be useful to indicate how the Justices' vote and thus the court outcome. These factors include the use of pleasant and unpleasant language by \textcite{black2011emotions}, the frequency and content of Justices' questions by \textcite{Shullman2004illusion} and \textcite{johnson2009inquiring}. \textcite{epstein2010inferring} use a regression analysis with the number of questions asked by the Justices used to infer the winning party in a case.

Other scholars \autocites{chen2016perceived}{chen2017covering}{schubert1992observing} have studied the emotion of the Justices from vocal characteristics and suggest that these vocal characteristics, especially perceived masculinity is strongly correlated with the court outcomes. \textcite{dietrich2019emotional} uses a multilevel logistic model with random effects to suggest that subconscious vocal inflections contain information that is not available from text.

Chen \autocite*{chen2018justice} employed both vocal and facial characteristics to predict the court votes using Supreme Court data from 1946-2014. The audio clips are first preprocessed to get the Mel-frequency Cepstral Coefficients (MFCC) and then applied to a random forest model. The image features are extracted using a Histogram of Oriented Gradients (HOG) method. More specific facial recognition software is readily available to extract human facial features and these facial recognition technologies have not yet been applied to the legal proceedings.

Most of the literature is conducted using the U.S. Supreme Court Database and less studies have been conducted using Australian High Court data. \textcite{tutton2018judicial} have used an ethnographic approach to study the transcript and AV recordings in the High Court of Australia but the study is conducted in an observational manner via matching the Justices' distinct behaviour with the transcript.

\hypertarget{facial-recognition}{%
\subsection{Facial recognition}\label{facial-recognition}}

An anatomical analysis of facial action \autocite{ekman1976measuring} led to the Facial Action Code (FAC) \autocite{ekman1978} and has been further revised by \textcite{ekman2002facial}. This decomposition of facial muscles is widely used in scientific research. It was applied in competitive sports, specifically tennis by \textcite{kovalchik2018going} who found that the emotion of professional tennis players will have an impact on their performance.

There have been many algorithms created for facial detection and the analysis of their performance when applied to images have been the focus of events like the Audio/Visual Emotion Challenge \autocites{schuller2012avec}{schuller2011avec} and Emotion Recognition In The Wild Challenge and Workshop \autocites{dhall2013emotion}{kahou2013combining}.

Facial recognition software has also been implemented by DeepFace \autocite{taigman2014deepface} from Facebook, and FaceNet \autocite{schroff2015facenet} from Google. OpenFace \autocite{baltrusaitis2018openface} is the first open-sourced face recognition software that provides facial expression detection, including facial landmarking, head pose estimation, eye gaze tracking and facial action unit detection. Along with its previous version (\textcite{baltruvsaitis2016openface}), the OpenFace toolkit has been used in different social research studies including depression classification (\textcite{yang2016decision} and \textcite{nasir2016multimodal}).

\hypertarget{data-collection}{%
\chapter{Data Collection}\label{data-collection}}

\hypertarget{data-processing}{%
\section{Data Processing}\label{data-processing}}

The source data for this research project is the AV recordings publicly available from the High Court of Australia \autocite{highcourtau}. Multiple procedures need to be performed to obtain the dataset.

The workflow for extracting numerical data from the videos can be found in Figure \ref{fig:workflow}. Youtube-dl \autocite{youtube-dl} has been used to download videos from the High Court of Australia\autocite{highcourtau} and the list of videos used in this research project is documented in the Appendix. Image frames are extracted from the videos for every minute via ffmpeg \autocite{ffmpeg}, resulting in 1021 image frames (252 frames from \texttt{Nauru} videos and 769 frames from other five videos). Taipan \autocite{Taipan} is then used to find the x-y coordinates of the location of the Justices in each image frame. ImageMagick \autocite{ImageMagick} is then used to crop the face of each Justice from each image frame that is taken from each video where three Justices present in \texttt{Nauru} videos and five Justices in other videos. The resulting 4601 cropped images (252 image frames from \texttt{Nauru} videos where three Justices are presented and 769 image frames from other videos where five justices are presented) are then sent to OpenFace \autocite{baltrusaitis2018openface} to produce the variables for facial landmarking, head pose, eye gaze and facial action unit. This step is performed via the docker platform. The resulting outputs from OpenFace are individual comma-separated values (csv) files for each of the 4601 faces considered and processing is done in R to combine all the separate csv files into a final dataframe with appropriate index of \texttt{frame\_id}, \texttt{judge\_id} and \texttt{video\_id}.

\begin{figure}
\includegraphics[width=1\linewidth]{figures/workflow} \caption{data processing workflow \label{fig:workflow}}\label{fig:unnamed-chunk-1}
\end{figure}

\hypertarget{variable-description}{%
\section{Variable description}\label{variable-description}}

OpenFace provides more than 711 variables measuring different aspect of a given face and a full description of the output variables can be found \href{https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units}{here}. This outlines the difficulty of this project: no existing models will present accurate prediction and inference using 700+ variables - how can we incorporate these information to say about the facial expressions of the Justices during the hearings?

I conduct some exploratory data analysis on one video: \texttt{Nauru\_a} and find the 700+ variables can be classified as follows with some insights

\begin{itemize}
\item
  \textbf{Confidence}: How confidence OpenFace is with the detection. Confidence is related to the angle that the Justice's face present in the images.
\item
  \textbf{Gaze}: Gaze tracking: the vector from the pupil to corneal reflection. The dataset contains information on the gaze for both eyes while there is no distinct difference between the eyes. Also I was trying to make animation to track the change of the gaze for judges but no good luck.
\item
  \textbf{Pose}: the location of the head with respect to camera. Pose-related variables don't provide much useful information apart from gaze-related variables.
\item
  \textbf{Landmarking}: landmarking variables for face and eyes. Landmarking variables allows me to plot the face of the judge in a particular frame. More work could be done to explore the usefulness of landmarking variables.
\item
  \textbf{Action Unit}: Action units are used to describe facial expressions. \href{https://imotions.com/blog/facial-action-coding-system/}{this website} provides a good animation on each action unit. The action unit has intensity measures ending with \texttt{\_c} and presence measures ending with \texttt{\_r}. These variables will be the focus of my project and a reference study of using action units to detect human emotion by Kovalchik can be found \href{http://www.sloansportsconference.com/wp-content/uploads/2018/02/2005.pdf}{here}.
\end{itemize}

\hypertarget{data-format}{%
\section{Data format}\label{data-format}}

In this project, we will make use of the action unit variables along with all the added indexes to analyse the face of the judge. In the wide format, apart from the first four index columns, each action unit has two columns with one for binary presence value and another for numeric intensity value. The Table \ref{tab:wide} presents the first five rows of the dataset with columns for the first action unit only.

\begin{table}[ht]
\begin{center}
\caption{\label{tab:wide} data in wide format}
\begin{tabular}{lllllllllllllllllllllllllllllllllllllll}
\toprule
judge & video & frame & AU01-r & AU02-r & AU04-r & AU05-r & AU06-r & AU07-r & AU09-r & AU10-r & AU12-r & AU14-r & AU15-r & AU17-r & AU20-r & AU23-r & AU25-r & AU26-r & AU45-r & speaker & AU01-c & AU02-c & AU04-c & AU05-c & AU06-c & AU07-c & AU09-c & AU10-c & AU12-c & AU14-c & AU15-c & AU17-c & AU20-c & AU23-c & AU25-c & AU26-c & AU28-c & AU45-c \\
\midrule
Bell & McKell & 1 & 0 & 0 & 0.69 & 0.63 & 0 & 1.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0.05 & 0 & 0 & 0.26 & 0.47 & Appellent & 0 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0 & 0 & 1 & 0 & 1 & 0 & 1 & 0 & 0 & 0 \\
Bell & McKell & 2 & 0 & 0 & 0.69 & 0.63 & 0 & 1.5 & 0 & 0 & 0 & 0 & 0 & 0 & 0.05 & 0 & 0 & 0.26 & 0.47 & Appellent & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

The data can also be expressed in the long format with action unit being another index and presence and intensity being two columns. The Table \ref{tab:long} presents the first five rows of the data in the long format.

\begin{table}[ht]
\begin{center}
\caption{\label{tab:long} data in long format}
\begin{tabular}{lllllll}
\toprule
judge & video & frame & speaker & AU & presence & intensity \\
\midrule
Bell & McKell & 1 & Appellent & AU01 &  1 & 0.00 \\
Bell & McKell & 1 & Appellent & AU02 &  1 & 0.00 \\
Bell & McKell & 1 & Appellent & AU04 &  0 & 0.69 \\
Bell & McKell & 1 & Appellent & AU05 &  1 & 0.63 \\
Bell & McKell & 1 & Appellent & AU06 &  0 & 0.00 \\
Bell & McKell & 1 & Appellent & AU07 &  1 & 1.54 \\
Bell & McKell & 1 & Appellent & AU09 &  1 & 0.00 \\
Bell & McKell & 1 & Appellent & AU10 &  1 & 0.00 \\
Bell & McKell & 1 & Appellent & AU12 &  0 & 0.00 \\
Bell & McKell & 1 & Appellent & AU14 &  0 & 0.00 \\
Bell & McKell & 1 & Appellent & AU15 &  1 & 0.00 \\
Bell & McKell & 1 & Appellent & AU17 &  0 & 0.00 \\
Bell & McKell & 1 & Appellent & AU20 &  1 & 0.05 \\
Bell & McKell & 1 & Appellent & AU23 &  0 & 0.00 \\
Bell & McKell & 1 & Appellent & AU25 &  1 & 0.00 \\
Bell & McKell & 1 & Appellent & AU26 &  0 & 0.26 \\
Bell & McKell & 1 & Appellent & AU28 & NA &   NA \\
Bell & McKell & 1 & Appellent & AU45 &  0 & 0.47 \\
Bell & McKell & 2 & Appellent & AU01 &  0 & 0.00 \\
Bell & McKell & 2 & Appellent & AU02 &  1 & 0.00 \\
Bell & McKell & 2 & Appellent & AU04 &  1 & 0.69 \\
Bell & McKell & 2 & Appellent & AU05 &  1 & 0.63 \\
Bell & McKell & 2 & Appellent & AU06 &  0 & 0.00 \\
Bell & McKell & 2 & Appellent & AU07 &  1 & 1.54 \\
Bell & McKell & 2 & Appellent & AU09 &  1 & 0.00 \\
Bell & McKell & 2 & Appellent & AU10 &  1 & 0.00 \\
Bell & McKell & 2 & Appellent & AU12 &  0 & 0.00 \\
Bell & McKell & 2 & Appellent & AU14 &  0 & 0.00 \\
Bell & McKell & 2 & Appellent & AU15 &  1 & 0.00 \\
Bell & McKell & 2 & Appellent & AU17 &  0 & 0.00 \\
Bell & McKell & 2 & Appellent & AU20 &  1 & 0.05 \\
Bell & McKell & 2 & Appellent & AU23 &  0 & 0.00 \\
Bell & McKell & 2 & Appellent & AU25 &  1 & 0.00 \\
Bell & McKell & 2 & Appellent & AU26 &  0 & 0.26 \\
Bell & McKell & 2 & Appellent & AU28 & NA &   NA \\
Bell & McKell & 2 & Appellent & AU45 &  0 & 0.47 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\hypertarget{missing-value-imputation}{%
\section{Missing value imputation}\label{missing-value-imputation}}

The missingness in the dataset could be due to the fact that a judge is reading the materials on the desk so the face is not captured for a particular frame or simply because some faces are not detectable for the given resolution of the video stream. However, since that data is in time series structure, simply drop the missing observation will cause the time interval to be irregular and complicate further analysis.

There are two different sets of variables that need imputation. \texttt{Presence} is a binary variable that takes value of one if an action unit is present in a particular frame for a judge in a video and \texttt{Intensity} measures how strong that action unit is. Linear interpolation from \texttt{forecast} package is suitable to impute \texttt{Intensity} and \texttt{Presence} is imputed through sampling from binomial distribution. The imputed action unit data is stored as \texttt{au\_imputed} under the \texttt{raw\_data} folder.

\hypertarget{data-cleaning}{%
\section{Data cleaning}\label{data-cleaning}}

There is a data quality issue coming from the data I get from OpenFace. For some observations, the intensity of the action unit could be high while the present variable has a zero value. This does not make sense since if an action unit has been detected as strong intensity for a judge in a particular frame, it should at least present on the judge's face. Therefore, I adjust for the presence value if the intensity is higher than one. One is being chosen as the threshold value since in Ekman's definition of the intensity of the action unit, a score of one means the action unit is at least slightly present in the judge's face. The adjusted data is stored as \texttt{au\_tidy} under the \texttt{raw\_data} folder.

\hypertarget{methods}{%
\chapter{Methods}\label{methods}}

Here you need to write about the analytical methods that you are using

\hypertarget{results}{%
\chapter{Results}\label{results}}

\hypertarget{notation}{%
\section{Notation}\label{notation}}

Let \(\mathbf{X}\) be a matrix of predictors, and \(\mathbf{Y}\) variable in our case is bivariate matrix of response variables, including a binary indicator of presence/absence and a numeric value measuring intensity, of facial action unit, where

\begin{itemize}
\tightlist
\item
  \(X_1\) indicates \texttt{judge} with six categories \(i = 1,2, \cdots, 6\)
\item
  \(X_2\) indicates \texttt{video} for each of the seven cases, \(j = 1,2, \cdots, 7\)
\item
  \(X_3\) indicates action unit containing 18 possible facial expression.\\
\item
  \(X_4\) indicates \texttt{speaker}, either the appellant or respondent, \(l=1,2\)
\item
  \(X_5\) indicates \texttt{frame} corresponding to time, \(t = 1,2, \cdots, T_j\)
\end{itemize}

Note that \(t\) could be considered a time variable, but because images are taken at 1 minute intervals, temporal dependence is unlikely to exist. Rather this should be considered an independent observation.

A full, main effects model for the data might be expressed as:

\[Y_{ijkl} = \mu + \alpha_i + \beta_j + \gamma_k + \delta_l + \varepsilon_{ijkl}\]

\noindent and we would be interested in interactions between judge, case, action unit and who is speaking. An alternative model structure, is to treat each action unit individually, and fit separate models.

Also, let \(P_{jitkl}\) represent the response variable presence, and \(I_{jitkl}\) represent the second response variable intensity. This notation will be helpful for defining the plots and models explained in this section.

\hypertarget{action-unit-presence}{%
\section{Action unit: Presence}\label{action-unit-presence}}

\hypertarget{mean-presence}{%
\subsection{Mean presence}\label{mean-presence}}

I first compute the average presence (\(P_{ik}\)) of each action unit for each judge as \[P_{ik} = \frac{\sum_{jt}X_{ijtk}}{\sum_{j = 1}^JT_j}\] This is then plotted in Figure \ref{fig:mean_presence} to give an overview of the presence of all the action units across all the judge. The order of action unit on the y axis is ranked by the average presence of all the judge. The five most frequent action units are highlighted in blue for each judge and summarised in Table \ref{tab:most_common}

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-4-1} \caption{The average presence score of each action unit for each judge, aggregating on video and time. \label{fig:mean_presence}}\label{fig:unnamed-chunk-4}
\end{figure}

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-5}\label{tab:most_common}The five most commonly presented action unit for each judge.}
\centering
\begin{tabular}{r|l|l|l|l|l|l}
\hline
index & Bell & Edelman & Gageler & Keane & Kiefel & Nettle\\
\hline
1 & AU15 & AU02 & AU02 & AU20 & AU02 & AU02\\
\hline
2 & AU09 & AU20 & AU05 & AU15 & AU25 & AU15\\
\hline
3 & AU25 & AU01 & AU15 & AU02 & AU45 & AU20\\
\hline
4 & AU02 & AU15 & AU14 & AU14 & AU20 & AU01\\
\hline
5 & AU01 & AU23 & AU20 & AU07 & AU14 & AU07\\
\hline
\end{tabular}
\end{table}

It can be seen that some of the action units are common across almost all the judges, these includes

\begin{itemize}
\tightlist
\item
  AU02 (outer eyebrow raise),
\item
  AU20 (lip stretcher),
\item
  AU15 (Lip Corner Depressor)
\item
  AU14 (Dimpler)
\end{itemize}

AU02 makes a contribution to surprise, which is a positive attitude showing that judges are interested in a particular moment \autocite{ekman2002facial}. According to \autocite{ekman2002facial}, AU14 indicates boredom and AU15 shows confusion. Along with other action units that presented with high frequency in a particular judge but not all (summarised in Table \ref{tab:other_highf}), the emotions judges displayed in the courtroom can be summarised into three categories, described in Table \ref{tab:three_category} along with the featured action units.

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-6}\label{tab:three_category} Summarised emotions and featured action units}
\centering
\begin{tabular}{l|l}
\hline
emotion & Featured Action Unit\\
\hline
Surprise & AU01, AU02, AU05\\
\hline
Boredom & AU14, AU23\\
\hline
Confusion & AU07, AU15, AU23\\
\hline
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-7}\label{tab:other_highf} Other high frequent action units }
\centering
\begin{tabular}{l|l|l|l}
\hline
judge & first & second & third\\
\hline
Bell & AU09: Nose wrinkler & AU25: Lips part & AU01: Inner brow raiser\\
\hline
Edelman & AU01: Inner brow raiser & AU23: Lip tightener & NA\\
\hline
Gageler & AU05: Upper lid raiser & NA & NA\\
\hline
Keane & AU07: Lid tightener & NA & NA\\
\hline
Kiefel & AU25: Lips part & AU45: Blink & NA\\
\hline
Nettle & AU01: Inner brow raiser & AU07: Lid tightener & NA\\
\hline
\end{tabular}
\end{table}

\hypertarget{model-fit}{%
\subsection{Model fit}\label{model-fit}}

The first model I use is a generalised linear model with binomial link to understand the presence of the action units. The variables used include the judge, action units and their interactions. The model can be written down as Equation \ref{eq:judge_au}.Judge Edelman and AU01 are selected as the base level.

\begin{equation}\label{eq:judge_au}
P_{ik} = \mu + \alpha_i + \gamma_k + (\alpha\gamma)_{ik}
\end{equation}

\begin{table}[t]

\caption{\label{tab:anova}\label{tab:anova}ANOVA result}
\centering
\begin{tabular}{l|r|r|r|r}
\hline
  & Df & Deviance & Resid. Df & Resid. Dev\\
\hline
NULL & NA & NA & 15183 & 18900.38\\
\hline
judge & 5 & 298.5383 & 15178 & 18601.84\\
\hline
AU & 3 & 1682.9621 & 15175 & 16918.88\\
\hline
judge:AU & 15 & 1113.8722 & 15160 & 15805.01\\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\begin{center}
\caption{\label{tab:result_1} model result}
\begin{tabular}{llllllll}
\toprule
judge & AU & prob & SE & df & asymp.LCL & asymp.UCL & .group \\
\midrule
Bell & AU14 & 0.15 & 0.0179 & Inf & 0.12 & 0.19 &  1             \\
Edelman & AU14 & 0.43 & 0.0155 & Inf & 0.40 & 0.46 &   2            \\
Kiefel & AU15 & 0.44 & 0.0210 & Inf & 0.40 & 0.48 &   2            \\
Edelman & AU15 & 0.46 & 0.0156 & Inf & 0.43 & 0.49 &   2            \\
Nettle & AU14 & 0.49 & 0.0201 & Inf & 0.45 & 0.52 &   23           \\
Kiefel & AU14 & 0.52 & 0.0211 & Inf & 0.48 & 0.56 &   234          \\
Gageler & AU20 & 0.55 & 0.0177 & Inf & 0.51 & 0.58 &    345         \\
Gageler & AU14 & 0.62 & 0.0173 & Inf & 0.58 & 0.65 &     456        \\
Gageler & AU15 & 0.62 & 0.0172 & Inf & 0.59 & 0.66 &      567       \\
Bell & AU20 & 0.62 & 0.0242 & Inf & 0.57 & 0.67 &     4567       \\
Keane & AU14 & 0.71 & 0.0227 & Inf & 0.66 & 0.75 &       678      \\
Bell & AU02 & 0.73 & 0.0223 & Inf & 0.68 & 0.77 &        78      \\
Nettle & AU20 & 0.74 & 0.0176 & Inf & 0.71 & 0.77 &         8      \\
Edelman & AU20 & 0.77 & 0.0132 & Inf & 0.74 & 0.79 &         89     \\
Keane & AU02 & 0.78 & 0.0207 & Inf & 0.74 & 0.82 &         890    \\
Nettle & AU15 & 0.78 & 0.0166 & Inf & 0.75 & 0.81 &         890    \\
Kiefel & AU20 & 0.84 & 0.0154 & Inf & 0.81 & 0.87 &          90    \\
Gageler & AU02 & 0.85 & 0.0125 & Inf & 0.83 & 0.88 &           0A   \\
Bell & AU15 & 0.86 & 0.0176 & Inf & 0.82 & 0.89 &          90A   \\
Keane & AU15 & 0.87 & 0.0170 & Inf & 0.83 & 0.90 &           0A   \\
Keane & AU20 & 0.93 & 0.0131 & Inf & 0.90 & 0.95 &            AB  \\
Edelman & AU02 & 0.95 & 0.0069 & Inf & 0.93 & 0.96 &             B  \\
Kiefel & AU02 & 0.95 & 0.0091 & Inf & 0.93 & 0.97 &             B  \\
Nettle & AU02 & 1.00 & 0.0028 & Inf & 0.99 & 1.00 &              C \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

I'm interested to know if the presence score for one pair of judge and action unit is signficiantly different from another pair. Aanalysis of Varaince (ANOVA) test result in Table \ref{tab:anova} shows that there are significant variance for both judge, AU and their interactions. The next step after ANOVA is to test how each level of judge and AU different from another and I use multiple comparison to do this. (manually it will be 861 test to perform since 6 judges and 7 au - using multiple comparison, we can perform this take while control for a relatively low 5\% false positive rate). The estimated coefficient for each judge and AU pair is reported in Table \ref{tab:result_1} along with the upper and lower confidence interval bond. The information in the group column is helpful to understand how one particular pair of judge and au is different from another pair. With compact letter display, the pair with the same letter/number are \emph{NOT} significantly different from each other. This result is also plotted in Figure \ref{fig:model_1_plot}.

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-9-1} \caption{THis is the graphical representation of model1\label{fig:model_1_plot}}\label{fig:unnamed-chunk-9}
\end{figure}

\hypertarget{presence-by-videos}{%
\subsection{Presence by videos}\label{presence-by-videos}}

Apart from visualising the general presence score for all the action units, I'm also interested in the break down statistics by video (\(P_{ijk}\)). This is computed as \[P_{ijk} = \frac{\sum_{t}X_{ijtk}}{T_j}\] for the four most common action units: AU02, AU14, AU15, AU20 and plotted in Figure \ref{fig:common_video}. From this plot, we can observe that some of the judge are have relatively stable display of action unit throughout different videos (i.e.~Edelman and Nettle), while Gagaler seems to be highly reactive to some cases (i.e.~OKS).

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-10-1} \caption{Average presence of the four most common action units for each judge by video\label{fig:common_video}}\label{fig:unnamed-chunk-10}
\end{figure}

\hypertarget{model-fit-1}{%
\subsection{Model fit}\label{model-fit-1}}

The second model as shown in Equation \ref{eq:judge_video} is estimated to understand the interaction effect between judge and video.

\begin{equation}\label{eq:judge_video}
P_{ijk} = \mu + \alpha_i + \beta_j +\delta_k + (\alpha\beta)_{ij}
\end{equation}

The estimated coefficients are presented in Table \ref{tab:result_2}

What we could find from Figure \ref{fig:model_2_plot}

\begin{itemize}
\tightlist
\item
  Judge Edelman, Keane and Kiefel behave relatively consistent throughout all the videos. Judge Gageler is also consistent throughout the trails except for video OKS.
\item
  Judge Nettle seems to have two different ``status''
\item
  Judge Bell behaves quite differently in the three videos she participates
\end{itemize}

\begin{table}[ht]
\begin{center}
\caption{\label{tab:result_2} model result 2}
\begin{tabular}{llllllll}
\toprule
judge & video & prob & SE & df & asymp.LCL & asymp.UCL & .group \\
\midrule
Bell & Parkes & 0.50 & 0.018 & Inf & 0.47 & 0.54 &  1         \\
Gageler & Rinehart-b & 0.56 & 0.044 & Inf & 0.47 & 0.65 &  123       \\
Edelman & OKS & 0.58 & 0.041 & Inf & 0.50 & 0.66 &  1234      \\
Gageler & Nauru-a & 0.62 & 0.021 & Inf & 0.58 & 0.66 &   2        \\
Gageler & Nauru-b & 0.66 & 0.025 & Inf & 0.61 & 0.71 &   234      \\
Edelman & Nauru-a & 0.67 & 0.020 & Inf & 0.63 & 0.71 &   234      \\
Gageler & Rinehart-a & 0.67 & 0.014 & Inf & 0.65 & 0.70 &   234      \\
Bell & McKell & 0.68 & 0.021 & Inf & 0.64 & 0.72 &   234      \\
Edelman & McKell & 0.68 & 0.021 & Inf & 0.64 & 0.72 &   234      \\
Edelman & Parkes & 0.68 & 0.016 & Inf & 0.65 & 0.71 &   234      \\
Edelman & Nauru-b & 0.68 & 0.024 & Inf & 0.63 & 0.73 &   2345     \\
Edelman & Rinehart-a & 0.68 & 0.013 & Inf & 0.66 & 0.71 &   234      \\
Edelman & Rinehart-b & 0.69 & 0.040 & Inf & 0.61 & 0.77 &   23456    \\
Kiefel & Rinehart-a & 0.71 & 0.013 & Inf & 0.68 & 0.73 &   2345     \\
Kiefel & Parkes & 0.74 & 0.015 & Inf & 0.70 & 0.76 &     456    \\
Gageler & McKell & 0.74 & 0.020 & Inf & 0.70 & 0.77 &    3456    \\
Nettle & Nauru-a & 0.74 & 0.018 & Inf & 0.71 & 0.78 &     456    \\
Nettle & Rinehart-a & 0.77 & 0.012 & Inf & 0.75 & 0.79 &      567   \\
Nettle & Nauru-b & 0.83 & 0.019 & Inf & 0.79 & 0.86 &       678  \\
Keane & McKell & 0.84 & 0.015 & Inf & 0.81 & 0.87 &        78  \\
Keane & Parkes & 0.85 & 0.012 & Inf & 0.83 & 0.87 &         8  \\
Keane & OKS & 0.86 & 0.026 & Inf & 0.80 & 0.90 &       6789 \\
Nettle & Rinehart-b & 0.88 & 0.026 & Inf & 0.82 & 0.92 &       6789 \\
Bell & OKS & 0.90 & 0.022 & Inf & 0.85 & 0.93 &        789 \\
Gageler & OKS & 0.97 & 0.011 & Inf & 0.94 & 0.99 &          9 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-11-1} \caption{THis is the graphical representation of model2\label{fig:model_2_plot}}\label{fig:unnamed-chunk-11}
\end{figure}

\hypertarget{appellant-vs.respondent}{%
\subsection{Appellant vs.~Respondent}\label{appellant-vs.respondent}}

The third model as shown in Equation \ref{eq:judge_speaker} is estimated to understand the interaction effect between judge and speaking party.

\begin{equation}\label{eq:judge_speaker}
P_{ijk} = \mu + \alpha_i + \beta_j +\gamma_k + \delta_l + (\alpha\delta)_{il}
\end{equation}

The estimated coefficients are presented in Table \ref{tab:result_3}

What we could find from Figure \ref{fig:model_3_plot}

\begin{itemize}
\tightlist
\item
  Judges are behaving pretty similar when different parties are talking
\end{itemize}

\begin{table}[ht]
\begin{center}
\caption{\label{tab:result_3} model result 3}
\begin{tabular}{llllllll}
\toprule
judge & speaker & prob & SE & df & asymp.LCL & asymp.UCL & .group \\
\midrule
Bell & Appellent & 0.60 & 0.021 & Inf & 0.56 & 0.64 &  1    \\
Bell & Respondent & 0.61 & 0.020 & Inf & 0.57 & 0.65 &  1    \\
Edelman & Appellent & 0.69 & 0.011 & Inf & 0.67 & 0.71 &   2   \\
Gageler & Appellent & 0.70 & 0.012 & Inf & 0.67 & 0.72 &   2   \\
Edelman & Respondent & 0.71 & 0.012 & Inf & 0.68 & 0.73 &   2   \\
Gageler & Respondent & 0.72 & 0.013 & Inf & 0.69 & 0.74 &   2   \\
Kiefel & Respondent & 0.73 & 0.014 & Inf & 0.70 & 0.75 &   2   \\
Kiefel & Appellent & 0.79 & 0.015 & Inf & 0.76 & 0.82 &    3  \\
Nettle & Respondent & 0.80 & 0.012 & Inf & 0.77 & 0.82 &    3  \\
Nettle & Appellent & 0.82 & 0.011 & Inf & 0.80 & 0.84 &    34 \\
Keane & Appellent & 0.83 & 0.015 & Inf & 0.80 & 0.86 &    34 \\
Keane & Respondent & 0.87 & 0.012 & Inf & 0.84 & 0.89 &     4 \\
\bottomrule
\end{tabular}
\end{center}
\end{table}

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-13-1} \caption{THis is the graphical representation of model3\label{fig:model_3_plot}}\label{fig:unnamed-chunk-13}
\end{figure}

\hypertarget{action-unit-intensity}{%
\section{Action unit: Intensity}\label{action-unit-intensity}}

\hypertarget{general-intensity-plot}{%
\subsection{General Intensity plot}\label{general-intensity-plot}}

In Ekman's 20002 FACS manual, the intensity of an action unit is defined based on five classes: Trace: 0-1, Slight: 1-2, Marked or pronounced: 2-3, Severe or extreme: 3-4 and Maximum: 4-5.

The boxplot of the intensity for all the judges across all the videos is presented in Figure \ref{fig:intensity}. Each bar-and-whisker represents the intensity (\(I_{ijtk}\)) of all the action units aggregated on time for a particular judge \(i\) in a specific case \(j\). For example, the first bar-and-whisker in case Nauru\_a is created using all the 17 action units of Edelman through out the elapsed time in Nauru\_a case.

From the plot, we can see that most of the action units have low intensity score and this is expected because usually judges are expected to behave neutral in the court room. Thus a square root transformation is taken on the y axis for better visualisation effect. We can find that Judge Nettle seems to have higher average in all the four cases he appears: Nauru\_a\&b, Rinehart\_a \&b.

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-14-1} \caption{General intensity score by judge and video\label{fig:intensity}}\label{fig:unnamed-chunk-14}
\end{figure}

\hypertarget{mean-intensity}{%
\subsection{Mean intensity}\label{mean-intensity}}

Mean intensity score (\(I_{ik}\)) of each action unit for each of the judge is computed as \[I_{ik} = \frac{\sum_{jt}X_{ijtk}}{\sum_{j = 1}^JT_j}\] and plotted in Figure \ref{fig:mean_intensity}. The five most intense action units for each judge are presented in Table \ref{tab:most_intense}. We can find that the common high intense action units includes

\begin{itemize}
\tightlist
\item
  AU20 (Lip Stretcher)
\item
  AU07 (Lid Tightener)
\item
  AU04 (Brow Lowerer)
\end{itemize}

AU04 also belongs to the confusion category as AU07. This could help to understand that judges are more likely to express a stronger confusing expression than other emotions.

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-16-1} \caption{Mean intensity score for each judge and action unit aggregating on videos.\label{fig:mean_intensity}}\label{fig:unnamed-chunk-16}
\end{figure}

\begin{table}[t]

\caption{\label{tab:unnamed-chunk-17}\label{tab:most_intense}The five most intense action unit for each judge.}
\centering
\begin{tabular}{r|l|l|l|l|l|l}
\hline
index & Bell & Edelman & Gageler & Keane & Kiefel & Nettle\\
\hline
1 & AU04 & AU20 & AU05 & AU20 & AU45 & AU20\\
\hline
2 & AU09 & AU06 & AU04 & AU07 & AU20 & AU01\\
\hline
3 & AU07 & AU17 & AU20 & AU15 & AU26 & AU07\\
\hline
4 & AU15 & AU07 & AU09 & AU14 & AU25 & AU06\\
\hline
5 & AU10 & AU04 & AU45 & AU17 & AU14 & AU17\\
\hline
\end{tabular}
\end{table}

\hypertarget{model-fit-2}{%
\subsection{Model fit}\label{model-fit-2}}

\hypertarget{intensity-plot-for-the-most-frequent-action-units}{%
\subsection{Intensity plot for the most frequent action units}\label{intensity-plot-for-the-most-frequent-action-units}}

Apart from visualising the general intensity score for all the action units, I'm also interested in the intensity score of the most frequent action units. Figure \ref{fig:intensity_by_au} presents this. The statistics being plotted is \(I_{ijtk}\) with \(k\) including AU02, AU14, AU15 and AU20 as the most common four action units. From this plot, we can learn that AU02, although being commonly detected for all the judges, has low intensity score.

\begin{figure}
\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-19-1} \caption{Intensity score of the most frequent action units, seperating by judge and video ID.\label{fig:intensity_by_au}}\label{fig:unnamed-chunk-19}
\end{figure}

\hypertarget{high-intensity-points}{%
\subsection{High intensity points}\label{high-intensity-points}}

We filter out the points have intensity greater than 2 (at least ``slight'' as per Ekman) in the previous plot and plot it against time and color by the speaker. It tells us that Edelman, Gageler and Nettle are the judges have stronger emotion that can be detected (since they have more points with intensity greater than 2). Different judges also have different time where they display stronger emotions. For example, Justice Nettle are more likely to have stronger emotion throughout the time when the appellant is speaking but only at the beginning and ending period when the respondent is speaking.

\includegraphics[width=1\linewidth]{thesis_files/figure-latex/unnamed-chunk-20-1}

\appendix

\hypertarget{additional-stuff}{%
\chapter{Additional stuff}\label{additional-stuff}}

\begin{longtable}[]{@{}lll@{}}
\caption{Details of videos processed.}\tabularnewline
\toprule
\begin{minipage}[b]{0.42\columnwidth}\raggedright
Case\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
Name\strut
\end{minipage} & \begin{minipage}[b]{0.35\columnwidth}\raggedright
AV recording link\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.42\columnwidth}\raggedright
Case\strut
\end{minipage} & \begin{minipage}[b]{0.14\columnwidth}\raggedright
Name\strut
\end{minipage} & \begin{minipage}[b]{0.35\columnwidth}\raggedright
AV recording link\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.42\columnwidth}\raggedright
Republic of Nauru v. WET040\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{Nauru\_a}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright
\url{http://www.hcourt.gov.au/cases/cases-av/av-2018-11-07a}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.42\columnwidth}\raggedright
TTY167 v. Republic of Nauru\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{Nauru\_b}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright
\url{http://www.hcourt.gov.au/cases/cases-av/av-2018-11-07b}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.42\columnwidth}\raggedright
Rinehart \& Anor v. Hancock Prospecting Pty Ltd \& Ors on 13 Nov 18\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{Rinehart\_a}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright
\url{http://www.hcourt.gov.au/cases/cases-av/av-2018-11-13}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.42\columnwidth}\raggedright
Rinehart \& Anor v. Hancock Prospecting Pty Ltd \& Ors on 14 Nov 18\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{Rinehart\_b}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright
\url{http://www.hcourt.gov.au/cases/cases-av/av-2018-11-14a}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.42\columnwidth}\raggedright
Parkes Shire Council v. South West Helicopters Pty Limited\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{Parkes}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright
\url{http://www.hcourt.gov.au/cases/cases-av/av-2018-11-14b}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.42\columnwidth}\raggedright
McKell v. The Queen\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{McKell}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright
\url{http://www.hcourt.gov.au/cases/cases-av/av-2018-12-07}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.42\columnwidth}\raggedright
OKS v. The State of Western Australia\strut
\end{minipage} & \begin{minipage}[t]{0.14\columnwidth}\raggedright
\texttt{OKS}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright
\url{http://www.hcourt.gov.au/cases/cases-av/av-2019-02-14}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{list-of-the-name-of-ction-units}{%
\section{List of the name of ction units}\label{list-of-the-name-of-ction-units}}

\begin{tabular}{l}
\hline
AU\_meaning\\
\hline
AU01: Inner brow raiser\\
\hline
AU02: Outer brow raiser\\
\hline
AU04: Brow lowerer\\
\hline
AU05: Upper lid raiser\\
\hline
AU06: Cheek raiser\\
\hline
AU07: Lid tightener\\
\hline
AU09: Nose wrinkler\\
\hline
AU10: Upper lip raiser\\
\hline
AU12: Lip corner puller\\
\hline
AU14: Dimpler\\
\hline
AU15: Lip corner depressor\\
\hline
AU17: Chin raiser\\
\hline
AU20: Lip stretcher\\
\hline
AU23: Lip tightener\\
\hline
AU25: Lips part\\
\hline
AU26: Jaw drop\\
\hline
AU28: Lip suck\\
\hline
AU45: Blink\\
\hline
\end{tabular}

\printbibliography[heading=bibintoc]



\end{document}
