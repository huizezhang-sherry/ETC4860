---
chapter: 1
knit: "bookdown::render_book"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message=FALSE, 
                      cache=TRUE, 
                      warning = FALSE,
                      out.width="100%", 
                      fig.width=10, 
                      fig.height=7)
# Load any R packages you need here
library(forecast)
library(tidyverse)
library(ggpubr)
library(knitr)
library(broom)
library(lsmeans)
library(multcomp)

load("data/au_tidy.rda")
load("data/most_common.rda")
load("data/au_imputed_old.rda")
load("data/au_meaning.rda")
```

# Introduction {#ch:intro}

## Statement of topic
Decisions by courtroom Justices have been discussed broadly in the legal literature. Gender, political views and religious background of both the Justices and counsel in the case potentially influence the decisions. This paper will explore the facial behaviour of the Justices during hearings with the objective of being to assess whether it can help to predict outcomes. Audio Visual(AV) recordings and case transcripts will be computationally processed and analysed to examine the decisions of each Justice. 

## Motivation

People have attempted to predict the decisions of the Justices in the past  century using judge characteristics i.e. Gender, political views, religious background. More recently, scholars[@Shullman2004illusion; @chen2018justice] have been using on-court information (i.e. AV recording, transcript, language used by the Justices) to predict the decision of the Justices using the U.S. Supreme Court data. On-court information has also been used to study data from High Court of Australia. @tutton2018judicial has used an ethnographic approach to present a observational study of judicial behaviour based on watching the audio footage. Manually observing the AV recordings could lead to subjective evaluation of facial expression and this motivates us to build upon @tutton2018judicial's work to employ facial recognition technology to study the facial expression of the justices, which will provide a more objective result than @tutton2018judicial.
 
## Literature review

The literature summary is divided into two parts: (1) current work in legal studies to understand the behaviour of the Justices and (2) existing facial recognition and emotion tagging technology.  

### Legal study from a behaviour perspective 

There is a large law & economics and political science literature that attempts to predict how judges will vote in court cases. Much of this focuses on the characteristics of the judge i.e. gender, political views, religious background and characteristics of the parties in the case i.e. gender or race of the defendant in criminal cases [@Stuart1962; @Peter1984; @Combining1987; @Susan1988; @Steffensmeier2001; @Kulik2003]. 

Moving from static information of the judge and parties involved, more studies start to incorporate the language used by the judge on the court to predict the decision of the Justices. @black2011emotions has study the use of pleasant and unpleasant language by the Justices and @Shullman2004illusion and @johnson2009inquiring have studied the effect of frequency and content of Justices' questions. @epstein2010inferring use a regression analysis with the number of questions asked by the Justices used to infer the winning party in a case. 

More recent legal study has focused on the usage of emotion and vocal characteristics of the Justices to predict the judge's vots. Although @judicalguid present the following code of conduct:

>It is important for judges to maintain a standard of behaviour in court that is consistent with the status of judicial office and does not diminish the confidence of litigants in particular, and the public in general, in the ability, the integrity, the impartiality and the independence of the judge. 

and this impartiality has been highlighted in judicial demeanour by  @tutton2018judicial and @goffman1956nature, Paul Ekman @ekman1991invited suggests that from a behavioural perspective, some facial and vocal inflections are often unbeknown to the speakers themselves. @chen2016perceived; @chen2017covering and  @schubert1992observing have studied the emotion of the Justices from vocal characteristics and suggest that these vocal characteristics, especially perceived masculinity is strongly correlated with the court outcomes. @dietrich2019emotional has used a multilevel logistic model with random effects to suggest that subconscious vocal inflections contain information that is not available from text. 

Moreover, a more sizeable study by @chen2018justice have incorporated both vocal and image information of the judge into a machine learning model to predict the court votes using the U.S. Supreme Court data from 1946-2014. He found that image features improved prediction of case outcomes from 64% to 69% and audio features improved prediction of case outcomes from 67% to 69%. This demonstrate the potential of incorporating facial information to understand the decision of the Justices. 

The literature mentioned above is mostly conducted using the U.S. Supreme Court Database and less studies have been conducted using Australian High Court data. @tutton2018judicial has used an ethnographic approach to study the judicial demeanour in the High Court of Australia  and it is the first of its kind to use transcript and AV recordings in Australia study. The study found that Justices present a detached facial demeanour during the court in most of the time while some human display of emotions i.e. laughter and humour have also been captured by the scholars. Tutton's work has confirmed the potential of using image information to understanding the Justices as in Chen's study, while the ethnographic approach could be biased and lead to subjective results when different people are observing the videos. Thus, building upon Tutton's study, my work fills the gap of producing objective result via utilising facial recognition technology. 



### Facial recognition

An anatomical study of the decomposition of facial muscles by [@ekman1976measuring] led to the devlopment of Facial Action Code (FAC) [@ekman1978] and identification of the six universal emotions on human faces. This work has been further revised as [@paulekmangroup] and has laid a solid foundation for analysing facial expression and developing facial recognition softwares for researchers [@Kobayashi1992; @huang1997; @lien2000; @Kappoor2003; @Tong2007; @Cohn2009; @Lucey2010]. 

To be able to analysis the facial expression, proper facial recognition technique is needed to first extract faces from images. Facial recognition softwares i.e. DeepFace [@taigman2014deepface] from Facebook and FaceNet [@schroff2015facenet] from Google have also been developed for face detection. OpenFace [@baltrusaitis2018openface] is the first open-sourced face recognition software that provides facial expression detection, including facial landmarking, head pose estimation, eye gaze tracking and facial action unit detection. The OpenFace toolkit has been used in different area in research including depression classification [@yang2016decision; @nasir2016multimodal], emotion study [@Pan2018; @Nasir2016; @Huber2018] and even sports analytics [@kovalchik2018going].


