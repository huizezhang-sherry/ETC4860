---
chapter: 2
knit: "bookdown::render_book"
---

# Data Collection

## Data Processing

The audio visual recordings of cases heard by the High Court of Australia [@highcourtau] are available on the High Court of Australia website.
Seven cases were selected, heard in the year 2018 they all were heard by either three or five of the seven judges. These videos displayed the judges faces above the required level of resolution of the OpenFace software, more than 30px. Appendix Table \ref{tab:case-info} contains a full list of the videos chosen for this study.

To analyse the facial expressions of the Justices the videos must be processed by OpenFace. To download videos from the High Court of Australia [@highcourtau] the software Youtube-dl [@youtube-dl] was used. Image frames were extracted from each of the videos, at every one minute interval via ffmpeg [@ffmpeg], this resulted in in 1021 image frames.

The Justices remain seated in the same position throughout the hearings, this means the same region of every image can be extracted to form a set of images containing each individual Justice. Taipan [@Taipan] is used to find the x-y coordinates of a box denoting the location of the Justices in each image frame. ImageMagick [@ImageMagick] was used to crop the face of each Justice from each image frame based on the coordinates from Taipan.

The resulting 4601 cropped regions containing Justice's faces are then sent to OpenFace [@baltrusaitis2018openface] to be processed. The results provided by OpenFace contained facial variables, these include facial landmarking, head pose, eye gaze and action units. The results are stored as separate comma-separated values (csv) files for each of the 4601 faces and post-processing is done in R to combine the separate csv files into a dataframe with additional index columns for frame, judge and video. 
The workflow to obtain the facial landmarks and expression information from the source videos has been displayed in Figure \ref{fig:workflow}. 

```{r fig.cap="This is an illustration of the workflow for extracting facial variables from videos. \\label{fig:workflow}"}
include_graphics("figures/workflow.png", dpi = 128)
```

## Variable description

OpenFace provides more than 711 variables measuring different aspects of a given face, a full description of the output variables can be found in @baltrusaitis2018openface. The facial variables can be summarised into the following categories. 

 - **Confidence**: How confident OpenFace is in the detection.
 
 <!--Confidence is related to the angle that the Justiceâ€™s face appears in the images. 
 -->
 - **Gaze**: Gaze tracking: the vector from the pupil to corneal reflection. The dataset contains information on the gaze for both eyes with no distinct difference between the eyes. 
 
<!--Also I was trying to make animation to track the change of the gaze for judges but no good luck. 
-->
 
 - **Pose**: the location of the head with respect to camera. 
 
 <!--Pose-related variables don't provide much useful information apart from gaze-related variables. 
 -->
 
 - **Landmarking**: location of facial landmark, for face and eyes. 
 
 <!--Landmarking variables allows me to plot the face of the judge in a particular frame. More work could be done to explore the usefulness of landmarking variables. 
 -->
 
 - **Action Unit**: Action units are used to describe facial expressions. The action unit has intensity measures ending with `_c` and presence measures ending with `_r`. 


## Data format

The data can be expressed in the long format with action unit as an index and presence and intensity presented as observations in two columns. Table \ref{tab:long} presents the data for Justices Edelman in video **McKell** for all the action units in the first frame in long format. Figure \ref{fig:ts-plot} plots the presence and intensity across time. 

```{r results='asis'}
long <- au_imputed %>%
        filter(judge == "Edelman", video == "McKell", frame <=1, AU != "AU28") 

knitLatex::xTab(format(as.data.frame(long), digits = 2), booktabs = TRUE, caption.top = "\\label{tab:long} This table is an illustration of a proportion of the data in long format.")

```


```{r ts-plot, fig.cap="The plot shows the intensity and presence score of action unit 01 for Justices Edelman in case McKell in a time series manner. "}
au_imputed %>% 
        filter(judge == "Edelman", video == "McKell", AU == "AU01") %>% 
        gather(type, value, c(presence, intensity)) %>% 
        ggplot(aes(x =frame, y = value, col = type)) + 
        geom_line() + 
        geom_point() + 
        facet_wrap(vars(type), nrow = 2, scales = "free")
```


## Missing value imputation 

The missing value structure in the dataset has been explored to examine possible relationships between the missing values and judicidal behaviour. 
This could be influenced by a judge reading materials on their desk, resulting in their face not being captured in a particular frame, or faces that are not detectable due to the given resolution of the video stream.
Simply drop the missing observations will cause the time interval to be irregular, to prevent this imputation is used. 

Intensity is a continuous variable that measures how strong the action unit is presented. The missing values of intensity are related to missing values in presence, they are zero when the action unit is not present, 1(present at minimum intensity) to 5(present at maximum intensity). Linear interpolation function (`na.interp()`) from `forecast` package is used to impute Intensity. The missing value of presence is then imputed based on if the intensity score of the missing observations are greater than 1. 

## Source Code 

It is difficult to source reliable facial data of the judges, the data processing procedure defined above is integral to sourcing and analysing a reliable data set. The workflow has been scripted is available in a github repository. This open source contribution enable reproducibility for this work should re-processing be needed, or more videos are available. This can also be used and extended by other researchers.




