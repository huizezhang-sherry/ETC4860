---
chapter: 2
knit: "bookdown::render_book"
---

# Data Collection

## Data Processing

The source data of this research project is the AV recordings publicly available from the High Court of Australia [@highcourtau]. Due to the requirement of resolution (more than 30px for face detection) of OpenFace, we picked up seven cases from 2018 that have less than seven judges as the sample videos for our dataset. A full list of video being processed can be found in Table \ref{tab:case-info} in the Appendix. 

Multiple procedures need to be performed to obtain the numerical value of facial variables from the source videos. The entire workflow has been plotted in Figure \ref{fig:workflow}. Youtube-dl [@youtube-dl] has been used to download videos from the High Court of Australia [@highcourtau]. Image frames are extracted from the videos for every minute via ffmpeg [@ffmpeg], resulting in 1021 image frames. Because the Justices are remain seated at the same position throughout the hearing, Taipan [@Taipan] is used to find the x-y coordinates of the location of the Justices in each image frame. ImageMagick [@ImageMagick] is followed to crop the face of each Justice from each image frame based on the coordinates from Taipan. The resulting 4601 cropped faces are then sent to OpenFace [@baltrusaitis2018openface] to process on Docker. The output of OpenFace contains facial variables like facial landmarking, head pose, eye gaze and  action unit. These outputs are stored as separate comma-separated values (csv) files for each of the 4601 faces and post-processing is done in R to combine all the separate csv files into a final dataframe with appropriate index of frame, judge and video added. 

```{r fig.cap="This is an illustration of the workflow for extracting facial variables from videos. \\label{fig:workflow}"}
include_graphics("figures/workflow.png", dpi = 128)
```

## Variable description

OpenFace provides more than 711 variables measuring different aspects of a given face and a full description of the output variables can be found in @baltrusaitis2018openface. The facial variables can be summarised into the following categories. 

 - **Confidence**: How confidence OpenFace is with the detection. Confidence is related to the angle that the Justiceâ€™s face present in the images. 
 
 - **Gaze**: Gaze tracking: the vector from the pupil to corneal reflection. The dataset contains information on the gaze for both eyes while there is no distinct difference between the eyes. Also I was trying to make animation to track the change of the gaze for judges but no good luck. 
 
 - **Pose**: the location of the head with respect to camera. Pose-related variables don't provide much useful information apart from gaze-related variables. 
 
 - **Landmarking**: landmarking variables for face and eyes. Landmarking variables allows me to plot the face of the judge in a particular frame. More work could be done to explore the usefulness of landmarking variables. 
 
 - **Action Unit**: Action units are used to describe facial expressions. The action unit has intensity measures ending with `_c` and presence measures ending with `_r`. 

In this project, we will focus on using action units along with information about judge, video, speaker to analyse the face of the judge.

## Data format

The data can be expressed in the long format with action unit being an index and presence and intensity being two columns. The Table \ref{tab:long} presents the data for Justices Edelman in video McKell for all the action unit in the first frame in long format. Figure \ref{fig:ts-plot} plots the presence and intensity across time. 

```{r results='asis'}
long <- au_imputed %>%
        filter(judge == "Edelman", video == "McKell", frame <=1, AU != "AU28") 

knitLatex::xTab(format(as.data.frame(long), digits = 2), booktabs = TRUE, caption.top = "\\label{tab:long} This table is an illustration of a proportion of the data in long format.")

```


```{r ts-plot, fig.cap="The plot shows the intensity and presence score of action unit 01 for Justices Edelman in case McKell in a time series manner. "}
au_imputed %>% 
        filter(judge == "Edelman", video == "McKell", AU == "AU01") %>% 
        gather(type, value, c(presence, intensity)) %>% 
        ggplot(aes(x =frame, y = value, col = type)) + 
        geom_line() + 
        geom_point() + 
        facet_wrap(vars(type), nrow = 2, scales = "free")
```


## Missing value imputation 

The missingness in the dataset could be due to the fact that a judge is reading the materials on the desk so the face is not captured for a particular frame or simply because some faces are not detectable for the given resolution of the video stream. Simply drop the missing observation will cause the time interval to be irregular and thus imputation is needed. 

Intensity is a continuous variable that measures how strong the action unit is presented. It is zero when the action unit is not present, 1(present at minimum intensity) to 5(present at maximum intensity). Linear interpolation function (`na.interp()`) from `forecast` package is used to impute Intensity. The missing value of presence is then imputed based on if the intensity score of the missing observations are greater than one. 

## Source Code 

The whole data processing procedure is done as a major part of the project and one of the difficulties of this project is actually to source reliable facial data of the judges, so that the subsequent modelling can then be conducted meaningfully. The whole workflow has been scripted and labelled accordingly from `data_01...` to `data_07...` on my github repository. If re-processing is needed, or more videos need to be added, it is straightforward for others to do. 





