Hi Everyone, Today I will be talking about exploring the judicial facial expression in videos of legal proceedings. You can go to this link to follow my talk. 

---

# Background

The videos of legal proceedings are available from the High Court of Australia, and here is a sample from The republic of Nauru v WET040.

[play video]

---

The Justices are expected to appear impartial in the courtroom. However, can you keep a straight face? Its hard not to react emotionally sometimes. 

There have been many studies into facial and vocal expressions of judges, based on court transcripts, or empirical studies. 

Chen published a paper in 2018 named Is Justice really blind? And it is also deaf. He processed the image of the attorneys before appearing on the US Supreme Court. Audio information is also included in Chen's data to classify if the supreme court judge would affirm or reverse a low court decision. Chen found that both the image and audio data improve the classification accuracy of case outcome. 

-- 

In Australia, Tutton et al used the transcripts and AV recordings of judges during the hearing to study the emotion of the judges in the High Court of Australia. They found that the judges behaved with a detached demeanour. Their video analysis was done with manual annotation. 

--

With the easy availability of facial recognition software it is interesting to automatically collect data on the facial expression of the human. In 2018, Kovalchik and Reid had used facial recognition software to study how the performance of professional tennis players is affected by their emotions in the Australian Open matches. 

---

In my research, I have completed the following tasks to study the emotion of the Justices from the High Court of Australia. 

I have extracted the facial expression of the justices from the videos. I have analysed the text transcript of the hearing to get the information of speakers. I then statistically model the judges facial expression using the data I processed to objectively study the emotion of the Justices. I want to find out, do my result agree or disagree with Tutton's findings, that the justices are appearing impartial. 

---

## Face recognition 

The facial recognition technology is based on the Facial action Coding System by Paul Ekman. It groups movements of facial landmarks into 45 action units. 
Here is an explanation of two action units.

Action unit 2 is the raising of outer eyebrow. This might be associated with surprise. 

Action unit 15 is the lip corner depressor. This might be associated with disgust.

---

## Data collection

The flowchart I'm going to show you outlines the steps to collecting the data.

The first step to collect the data is to process the videos. The videos are downloaded from the high Court of Australia website, and chopped into a set of images extracted at 1 minute intervals. This produces 1000 frames. The face recognition is conducted on these still images. 

Faces are extracted from each image. Because the Justices are remain seated in the same position through out the hearing, their faces can be easily extracted by cropping a fixed region of the image. This yields 4600 images total. 

The images are all processed with the face recognition software which tags each face with the facial action unit presence and intensity, along with many other variables like landmarks and eye focus. 

The information is collected into one csv file that contains 711 facial variables. 

One additional step is to analyse the transcript to extract times when the appellant and respondent were addressing the judges. 

---

The collected dataset contains 7 videos of 6 judges and 18 action units. Other facial variables obtained includes the coordinates of 68 face landmarks and 56 eye landmarks. In total, I processed 1000 image frames and 4600 faces. 
---

The final data format looks like this. We have judge_id, video_id, frame_id for each face. Speakers (appellant or respondent) is extracted from the text transcript. There are 18 action units and presence/absence of the action unit, and intensity score for the action unit.  

---

## Method

To be able to model the facial expression of the judge, the following notations are defined. $i$ is the subscript for judge and can take value from 1 to 6 for our 6 different judges. Video is represented by $j$ subscript and have 7 distinct values. Action unit is represented by $k$, speakers is subscripted by $l$. There is also a time variable indicating the 1 minute interval. 

The response variable used for this model is the binary presence variable, indicating whether the action unit is observed or not. 

---

## Modelling

The judges facial expressions are modelled using a binomial model with logit link. 

alpha_i, beta_j, gamma_k and delta_l represent the main effect of judge, video, action unit and speaker, respectively.

The interaction term between judge (alpha) and video (beta_j) allows different judges to react differently in different videos. Similarly we have interactions between judge and action unit, judge and speaker, and action unit and speaker. [Maybe more explanation on the interaction terms??] 

--

With the model, we are able to answer the following two questions: Do the  expression of the justices differ from case to case and Do the expression of the justices differ when different parties are speaking. 

---

# Result 

Here I present the first 10 rows of the model summary output. Because we include the interaction terms in the model, this output doesn't allow us to explicitly interpret the estimates. To overcome this, I will talk about the post-model analysis shortly. 

--
## ANOVA

The ANOVA test suggests that the variance of both the main effect and interactions are significant. However, since judge, video and AU are multi-level factors, ANOVA doesn't tell us which level of the variable has variance significantly different from others. Therefore, I apply multiple comparison to adjust the confidence interval. 

---

## Estimated marginal mean

As I mentioned before, the interpretation with the previous model summary is problematic, I calculate the estimated marginal mean over a pre-defined reference grid. In our model, the reference grid is created via the three way interactions of the 6 judges, 7 videos and 4 main action units and exclude the interactions that do not exist. For example, Judge Bell doesn't participated in case Nauru-a, so there will not be an interaction of judge bell and case nauru-a. 

[Do i need to explain how is the estimate calculated??]
Take 0.93 as an example. Because we use Edelman, McKell and AU02 as the base level for our factors. The only relevant estimate from the previous table is the intercept, which is 2.61. Supply this to the link function for the binomial model will give 0.93, which is the estimated marginal mean. 


---

## simultaneous confidence interval

The confidence interval also need to be adjusted to for multiple comparison to ensure the confidence level of the whole group is 95%. In my context, it is to ensure the judge-wise confidence is 95%. To do this, I use Bonferroni adjustment. [Do I need to further explain how does bonferroni adjustment do this??? Maybe put as an appendix slide??]

---

I then plot the 95% confidence intervals computed by multiple comparison of the means after the model fitting.

The x axis represents video and y axis represents the proportion of frames in each video where the action unit was present. The facets show judge in the columns, and four main action units on the rows. Colour represents video. 

Note that, since we have an unbalanced dataset, each case may have different judges.

There are so many possible findings to report. Here are the main ones. 

The facial expressions of Judge Edelman, Keane and Kiefel were relatively consistent throughout all the videos they participated. 

Judge Gageler reacted differently in the OKS case, on the four main action units. 

In the same OKS case, Judge Bell also shows each expression with a higher proportion. 

---

## Summary of results

Our results mostly validate Tutton et al (2018) that the judges appear impartial, with the exception of Gageler and Bell for the OKS case.

An important emphasis of this work is that "Facial recognition analysis of the videos provides a way to objectively assess judicial behaviour."

---

# Future work

Now, I will now briefly discuss the future work and some of the caveats of my work.

In the future, additional modelling could be done to model the presence separately for each judge. [need more stuff here]


More work could also be done to model the intensity score of the action units. When plotting the histogram of the intensity for each judge, we can see that most of the intensity score is quite low and near zero. a zero inflated model would be able to handle this type of data. 

---

# Caveats

There are two concerns the current work. Firstly, the image frames are extracted at every 1 minute interval. However, we would expect some facial expressions may only last for a few second. Thus more frequent time interval could be used to get more precise facial information of the judges. 

In my work, 7 videos are being processed into the facial recognition software.  More videos could be processed in the workflow I layed out to get more robust results. The reason for me not processing more videos is because the resolution of publicly available videos from the high court has only 720 pixels while the facial recognition software requires at least 30 pixels for a face to be detected. 

However, my work has established a workflow for extracting facial expressions of human from videos. All the scripts are available from my github repo. 


---

## Packages used in the research include

This is the list of software and R packages used for this work.

# Acknowledgement

Thank you for listening


