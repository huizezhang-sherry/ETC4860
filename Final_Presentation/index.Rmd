---
title: "Exploration of Judicial Facial Expression in Videos of Legal Proceedings"
author: "Sherry Zhang" 
institution: "Monash University"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["presdefault.css"]
    lib_dir: libs
    seal: FALSE
    nature:
      ratio: '16:9'
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE,
                      cache = TRUE, 
                      fig.path = "Figures/", 
                      out.height = "500px", 
                      out.width = "700px", 
                      results = "asis")

library(knitr)
library(emmeans)
library(tidyverse)
library(car)

load("../raw_data/au_tidy.rda")
load("../raw_data/au_meaning.rda")
```

class: title-slide, center, middle

# `r rmarkdown::metadata$title`

### `r rmarkdown::metadata$author` 

### [`r icon::fa_twitter()`](https:://twitter.com/huizezhangsh) huizezhangsh [`r icon::fa_github()`](https:://github.com/huizezhang-sherry) huizezhang-sherry

###`r rmarkdown::metadata$institution`

### https://judgeface.netlify.com/Final_Presentation

---
class: center
[![A video for fun](../images/video_photo.png)](https://www.youtube.com/watch?v=Ae7uKzin0GA)

### Are the justices' facial expressions revealing the outcome of the case?

---
# Previous work

**Judges are expected to be impartial in expressions presented to court.** 

- Chen (2018) 
    - Is Justice really blind? And it is also deaf?
    - **Data collection**: images of lawyers *before* appearing on the Supreme Court, voice audio, augment data on judges and case characteristics 
    - **Finding**: Image and vocal data improved accuracy of predicting case outcome
--

- Tutton, Mack, Roach Anleu (2018)
    - Empirical study of transcripts and AV recordings of judges in High Court cases
    - **Data collection**: Manual tagging of video footage of Australian High Court
    - **Finding**: judges present an impersonal or detached demeanour
--

- Kovalchik and Reid (2018)
    - Use automatic tagging of expressions from facial recognition software to study the emotion of tennis players in professional matches, focusing on the effect on performance


---
# My research

- Extract facial expression data of the Justices from videos of High Court of Australia

--

- Merge with data from text transcripts

--

- Statistically model judges facial expressions

--

- Provide an **objective** source of data to study the problem

--

- Do the results agree or disagree with Tutton's findings, that the justices are appearing impartial?

---
# Face Recognition


- **Paul Ekman**: Facial Action Coding System (FACS)


- it groups movements of facial landmarks into 45 **action units** 


.pull-left[

AU02 -	Outer eyebrow raiser

```{r AU02, echo = FALSE, out.height="200px", out.width="350px"}
include_graphics("../images/AU2-right-only.gif")
```
]


.pull-right[

AU15 - Lip corner depressor

```{r AU15, echo = FALSE, out.height="200px", out.width="350px"}
include_graphics("../images/AU15.gif")
```
]

 - widely used in software development and human emotion research

---

class: inverse, center, middle

# Video Processing

---

```{r workflow_1, echo = FALSE, fig.align="center"}
include_graphics("../images/workflow_1 (0).png")
```

---

```{r workflow_2, echo = FALSE, fig.align="center"}
include_graphics("../images/workflow_1 (1).png")
```

---

```{r workflow_3, echo = FALSE, fig.align="center"}
include_graphics("../images/workflow_1 (2).png")
```

---

```{r workflow_4, echo = FALSE, fig.align="center"}
include_graphics("../images/workflow_1 (3).png")
```

---

```{r workflow_5, echo = FALSE, fig.align="center"}
include_graphics("../images/workflow_1 (4).png")
```

---

```{r workflow_6, echo = FALSE, fig.align="center",}
include_graphics("../images/workflow_1 (5).png")
```

--

### 4601 faces and 711 facial variables.

---
class: middle, center 

# Collected data

####**7 videos**

####**6 judges**

####**18 action units**

####68 face landmarks, 56 eye landmarks

####1000 total frames, 4600 faces

---
class: middle, center


```{r data-format, echo = FALSE, warning=  FALSE}
include_graphics("../images/long.png")

```

---
class: inverse, center, middle

#  Method


---

## Notation: 

|Variable|Range|
|---|----|
|Judge| $i = 1,2, \cdots, 6$ |
|Video| $j = 1,2, \cdots, 7$ |
|Action unit| $k = 1, 2, \cdots, `r length(unique(au_tidy$AU))`$ possible facial expression|
|Speaker| $l = 1,2$: either the appellant or respondents|
|Frame| $t = 1,2, \cdots, T_j$ |
|Presence |The binary $Y$ variable:  $P_{ijkl}$|


<!-- - $X_1$ indicates `judge` with six categories $i = 1,2, \cdots, 6$ -->
<!-- - $X_2$ indicates `video` for each of the seven cases, $j = 1,2, \cdots, 7$ -->
<!-- - $X_3$ indicates action unit containing `r length(unique(au_tidy$AU))` possible facial expression.   -->
<!-- - $X_4$ indicates `speaker`, either the appellant or respondent, $l=1,2$ -->
<!-- - $X_5$ indicates `frame` corresponding to time, $t = 1,2, \cdots, T_j$ -->
<!-- - $P_{ijkl}$ indicates the binary `presence` variable  -->



---

## Modelling:

Binomial model with logit link: 

\begin{aligned}
P_{ijklt} &= \frac{e^{\eta_{ijklt}}}{1 + e^{\eta_{ijklt}}} \\
\eta_{ijklt} &= \mu + \alpha_i + \beta_j +\gamma_k + \delta_l + (\alpha\beta)_{ij} + (\alpha\gamma)_{ik} + (\beta\gamma)_{jk} + (\alpha\delta)_{il} + \varepsilon_{ijklt}
\end{aligned}

where

- $\mu$ sets an overall mean

- $\alpha_i$, $\beta_j$, $\gamma_k$ and $\delta_l$ represent the main effect of judges, video, action unit and speaker, respectively

- The interaction term between judge ( $\alpha_i$) and video ( $\beta_j$) allows different judges to react differently in different videos. Similarly for judge and action unit, judge and speaker, and action unit and speaker. 

--

## **Do the justices' expression differ from case to case?** 


---
class: inverse, center, middle

#  Result

---



.pull-left[

## Model summary
```{r summary-table, echo = FALSE}
model_dt <- au_tidy %>% 
  ungroup(judge) %>% 
  mutate(judge = fct_relevel(judge, "Edelman"), 
         AU = fct_relevel(AU, "AU01")) 

model_dt_2 <- model_dt %>% 
  filter(AU %in% c("AU02", "AU14", "AU20","AU15")) 

binomial_model_2 <- glm(presence ~ judge*video + judge*AU + video*AU, 
                        family = binomial(link = "logit"),  
                        data = model_dt_2)

tidy(binomial_model_2) %>% 
  head(10) %>% 
  kable(format = "html", digits = 2)
```

]

--
.pull-right[

## ANOVA 

```{r ANOVA, echo = FALSE}
Anova(binomial_model_2, type = "III", singular.ok = TRUE) %>% 
  kable(format = "html", digits = 5) %>% 
  kableExtra::column_spec(4, background = "#FFC0CB")
```

]

---

### Estimated Marginal Mean (EMM)

```{r emmean, echo = FALSE}
emmean_obj_2 <- emmeans(binomial_model_2, c("judge", "video", "AU"), 
                         type = "response") 

emmean_obj_2 %>% 
  as.data.frame() %>% 
  filter(!is.na(prob)) %>% 
  select(-df) %>% 
  head(10) %>% 
  kable(format = "html", digits = 4) %>% 
  kableExtra::column_spec(4, background = "#FFC0CB")
```

---

### Simultaneous confidence interval 

```{r ci-adjustment, echo = FALSE}
int_2 <- confint(emmean_obj_2, adjust = "bonferroni")

int_2 %>% 
  as.data.frame() %>% 
  filter(!is.na(prob)) %>% 
  select(-df) %>% 
  head(10) %>% 
  kable(format = "html", digits = 4) %>% 
  kableExtra::column_spec(c(6,7), background = "#FFC0CB")
```

---

```{r model-plot, echo = FALSE, message = FALSE, warning = FALSE, fig.width=16, fig.height=9, out.width="100%", out.height="100%"}

int_2 %>% 
  filter(!is.na(df)) %>% 
  mutate(judge = fct_relevel(judge, c("Edelman", "Keane", "Kiefel", "Nettle", "Gageler", "Bell"))) %>% 
  ggplot(aes(x= fct_relevel(video, c("Nauru-a", "Nauru-b", "Rinehart-a",
                               "Rinehart-b", "McKell", "OKS", "Parkes")), 
                            y = prob,  group = judge)) + 
  geom_point(aes(col= video), size = 2) + 
  geom_line(alpha = 0.5, lty = "dashed") + 
  geom_errorbar(aes(ymin = asymp.LCL, ymax = asymp.UCL, col= video), 
                width = 0.2, size = 2) + 
  facet_grid(AU ~ judge, scales = "free") + 
  theme(legend.position = "bottom", 
        legend.text = element_text(size = 30),
        axis.title = element_text(size=  40),
        axis.text = element_text(size = 20),
        axis.text.x = element_text(angle = 90, hjust = 1, size= 25),
        strip.text.x = element_text(size = 20),
        strip.text.y = element_text(size = 20)) + 
  xlab("video") + 
  ylab("Proportion")
  
```

---
# Discussion

- Generally the judges expressions are the same regardless of the video

- Nettle always has a raised outer eyebrow (AU02)

- Gageler and Bell reacted strongly to the OKS case, but Edelman and Keane did not

--

 **Our results mostly validate Tutton et al (2018) that the judges appear impartial, with the exception of Gageler and Bell for the OKS case.**

--

 #### **Facial recognition analysis of the videos provides a way to *objectively* assess judicial behaviour.** 

---

# Future work

.pull-left[

- Additional models: model each jduge or action unit separately

- Model intensity score: zero inflated model


]


.pull-right[
```{r intensity, echo = FALSE, fig.height=9, fig.width = 12, out.height="100%", out.width="100%"}
au_intensity_all <- au_tidy %>% 
  mutate(is_intense = ifelse(intensity >= 2, 1, 0))  

model_dt %>% 
  ggplot(aes(x = intensity)) + 
  geom_histogram() + 
  facet_wrap(vars(judge)) + 
  theme(axis.text = element_text(size = 30), 
        strip.text = element_text(size = 30), 
        axis.title = element_text(size = 30))
```

]


---

# Caveats about my work: 

**More frequent interval**: 
- current frame extraction is conducted on the 1 minute interval
- more frequent time interval i.e. 5 second,  could be used to catch more precise emotions

**More videos**
- 7 videos are processed by the facial recognition software
- more videos could be processed if higher resolution videos are accessible

My work has established a workflow for extracting facial expressions of human from videos. All the scripts are available from my github repo: 

<center>

#### https:://github.com/huizezhang-sherry/ETC4860 


---

# Packages used in the research: 


- [**youtube-dl**](https://ytdl-org.github.io/youtube-dl/index.html) for video downloading

- [**ffmpeg**](https://www.ffmpeg.org/) for video processing

- [**taipan**](https://github.com/srkobakian/taipan) R package for image annotation

- [**ImageMagick**](https://imagemagick.org/index.php) for image processing

- [**OpenFace**](https://github.com/TadasBaltrusaitis/OpenFace) for facial recognition

- [**tidyverse**](https://CRAN.R-project.org/package=tidyverse) suite of R packages for data manipulation and plotting

- [**emmeans**](https://CRAN.R-project.org/package=emmeans) R package for computing estimated marginal mean 

- [**car**](http://socserv.socsci.mcmaster.ca/jfox/Books/Companion) R package for ANOVA test. 
---
class: center, middle

# Acknowledgements

I would like to express my gratitude to Di Cook and Russell Symth for supervising this work, and 

Stephanie Kobakian and Stuart Lee for ad-hoc helps. 


Slides created via the R package [**xaringan**](https://github.com/yihui/xaringan).

---

## Image processing: Chen (2018)

```{r HOG, echo = FALSE, out.width="500px", fig.align= "center"}
include_graphics("../images/HOG.png")
```

