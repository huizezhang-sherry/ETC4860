Hi Everyone, Today I will be talking about exploring the judicial facial expression in videos of legal proceedings. You can go to this link to follow my talk. 

---

# Background

The videos of legal proceedings are available from the High Court of Australia, and here is a sample from The republic of Nauru v WET040.

[play video]

---

The Justices are expected to appear impartial in the courtroom. However, can you keep a straight face? Its hard not to react emotionally sometimes. 

There have been many studies into facial and vocal expressions of judges, based on court transcripts, or empirical studies. 

Chen published a paper in 2018 named Is Justice really blind? And it is also deaf. He processed the image of the attorneys before appearing on the US Supreme Court. Audio information is also included in Chen's data to classify if the supreme court judge would affirm or reverse a lowcourt decision. Chen found that both the image and audio data improve the classification accuracy of case outcome. 

-- 

In Australia, Tutton et al used the transcripts and AV recordings of judges during the hearing to study the emtion of the judges in the High Court of Australia. They found that the judges behaved with a detached demeanour. Their video analysis was done with manual annotation. 

--

With the easy availability of facial recognition software it is interesting to automatically collect data on the facial expression of the human. In 2018, Kovalchik and Reid had used facial recognition software to study how the performance of professional tennis players is affected by their emotions in the Australian Open matches. 

---

In my research, I have completed the following tasks to study the emotion of the Justices from the High Court of Australia. 

I have extracted the facial expression of the justices from the videos. I have analysed the text transcript of the hearing to get the information of speakers. I then statistically model teh judges facial expression using the data I processed to oebjectively study the emtion of the Justices. I want to find out, do my result agree or disagree with Tutton's findings, that the justices are appearing impartial. 

---

## Face recognition 

The facial recognition technology is based on the Facial action Coding System by Paul Ekman. It groups movements of facial landmarks into 45 action units. 
Here is an explanation of two action units.

Action unit 2 is the raising of outer eyebrow. This might be associated with surprise. 

Action unit 15 is the lip corner depressor. This might be associated with disgust.

---

## Data collection

The flowchart I'm going to show you outlines the steps to collecting the data.

The first step to collect the data is to process the videos. The videos are downloaded from the high Court of Australia website, and chopped into a set of images extracted at 1 minute intervals. This produces 1000 frames. The face recognition is conducted on these still images. 

Faces are extracted from each image. Because the Justices are remain seated in the same position through out the hearing, their faces can be easily extracted by cropping a fixed region of the image. This yields 4600 images total. 

The images are all processed with the face recognition software which tags each face with the facial action unit presence and intensity, along with many other variables like landmarks and eye focus. 

The information is collected into one csv file that contains 711 facial variables. 

One additional step is to analyse the transcript to extract times when the appellant and respondent were addressing the judges. 

---

The collected dataset contains 7 videos of 6 judges and 18 action units. Other facial variables obtained includes the coordinates of 68 face landmarks and 56 eye landmarks. In total, I processed 1000 image frames and 4600 faces. 
---

The final data format looks like this. We have judge_id, video_id, frame_id for each face. Speakers (appellant or respondent) is extracted from the text trascript. There are 18 action units and presence/absence of the action unit, and intensity score for the action unit.  

## Method

In the sample, we have 6 different judges, 7 different cases and 18 different action units. 

Speaker is a binary variable that indicating whether the appellant or the respondent is speaking. 

There is also a time variable indicating the 1 minute intervals. 

The response variable used for this model is the binary presence variable, indicating whether the action unit is observed or not. 


## Modelling

The judges facial expressions are modelled using a generalised linear model. 

alpha_i, beta_j, gamma_k and delta_l represent the effect of judge, video, action unit and speaker, respectively

The interaction term between judge (alpha) and video (beta_j) allows different judges to react differently in different videos. Similarly for judge and action unit, judge and speaker, and action unit and speaker. 


With the model, we are able to answer the following two questions: Do the justices' expression differ from case to case and Do the justices' expression differ when different parties are speaking. 

After the model fit, multiple comparisons are conducted to compare specific effects.


## Result 


This is a plot of the 95% confidence intervals computed by multiple comparison of the means after the model fitting. Bonferroni adjustments were made.

The x axis represents video and y axis represents the proportion of frames in each video where the action unit was present. The facets show judge in the columns, and four main action units on the rows. Colour represents video. 

Note that, each case may have different judges. Not all judges sit on each case.

There are so many possible findings to report. Here are the main ones. 

The facial expressions of Judge Edelman, Keane and Kiefel were relatively consistent throughout all the videos they participated. 

Judge Gageler reacted differently in the OKS case, on the four main action units. 

In the same OKS case, Judge Bell also shows each expression with a higher proportion. 


## Summary of results

Our results mostly validate Tutton et al (2018) that the judges appear impartial, with the exception of Gageler and Bell for the OKS case.

An important emphasis of this work is that "Facial recognition analysis of the videos provides a way to objectively assess judicial behaviour."


---

I will now briefly discuss some of the caveats of my work. The current image frames are extracted at every 1 minute interval. However, we would expect some facial expression may only last for a few second. Thus more frequent time interval could be used for getting more precise facial information of the judges. 

In my work, 7 videos are being processed into the facial recognition software and more videos could be processed in the workflow I layed out to get more robust results. The reason for me not processing more videos is because the resolution of publicly available vidoes from the high court has only 720 pixels while the facial recognition software requires at least 30 pixels for a face to be detected. This means that our software will have some problems detecting faces from videos that have 7 judges. 

However, my work has established a workflow for extracting facial expressions of human from videos. All the scripts are available from my github repo. 


---

## Packages used in the research include

This is the list of software and R packages used for this work.

# Acknowledgement

Thank you for listening


here are many more variables created including where the eyes are looking, and facial landmarks, but we focus only on the action units.
