---
title: "Collecting data"
author: "Huize Zhang"
date: "13/03/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(jsonlite)
library(rvest)

library(imager)
library(magick)

library(purrr)
library(tidyverse)
library(ggplot2)
library(GGally)
library(gganimate)
```

# Documentation
## OpenFace 
- OpenFace(https://cmusatyalab.github.io/openface/) is a broad application to detect face  
- notice that there are two version of it in github: "cmusatyalab/openface" and "TadasBaltrusaitis/OpenFace". 
- There is also a docker version  https://hub.docker.com/r/bamos/openface/

## OpenCV
- OpenCV(https://opencv.org/opencv-4-0-0.html) looks to be a good tool to crop face from image while a lot of the example/ tutorials are in C++ or python
- It has its own version in R called `ROpenCVLite`. I manage to get it installed: "swarm-lab/ROpenCVLite", but have no idea how to use it - haven't found tutorial/ documentation on that: https://swarm-lab.github.io/ROpenCVLite/

## OpenPose
- CMU-Perceptual-Computing-Lab/openpose
- https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/standalone_face_or_hand_keypoint_detector.md

## video/ image
- google can't really recognise face if it is the case of 7 judges because faces are too small and the video is only 720p
- I manage to get the data from the faces that google recognised in a desiable format - see the chunk below
- find that usually judges are looking down because the lawyer will guide them to look at certain pages of the book. Only when one or more of the judge is asking a question, most of the judge will look up to the camera. 


## Further for week 3
using r script to take screenshot of the media 
- ffemeg(looks doable):https://www.ffmpeg.org/ffprobe.html 
- PhantomJS
- R: webshot, takescreenshot
- have a look at how to do openpose 

# Google API
```{r eval = FALSE}

dt <- jsonlite::fromJSON("goog_oks.json")

landmark <- dt$faceAnnotations$landmarks
names(landmark) <- paste0("face_",rep(1:length(landmark)))

landmark2 <- list(landmark[["face_1"]][["position"]],
                  landmark[["face_2"]][["position"]],
                  landmark[["face_3"]][["position"]])

face <- data.frame(matrix(unlist(landmark2), nrow=length(landmark2), 
                          ncol =nrow(landmark2[[1]]),byrow=T))

names(face) = landmark[[1]]$type
face_oks <- face %>% mutate(ID = seq(1:nrow(face)))

```

# Scraping the list of video
```{r eval = FALSE}
url = "http://www.hcourt.gov.au/cases/recent-av-recordings"
webpage = read_html(url)
nodes<- html_nodes(webpage, xpath = '//*[@id="container"]/div[3]/div[1]/table')
tbody <- html_table(nodes)
recording <- data.frame(tbody)
names(recording) <- recording[1,]
recording <- recording[-c(1,nrow(recording)),-c(4)]
```

## week 3 Task
using r script to take screenshot of the media 
- ffemeg(looks doable):https://www.ffmpeg.org/ffprobe.html 
- PhantomJS
- R: webshot, takescreenshot

# webshot() and takescreenshot() from R
`webshot()` can take a screenshot of the website but only for static website. Don't think can do it for videos

```{r eval = FALSE}
library(webshot)
webshot("https://www.r-project.org/", "r.png")
webshot("https://www.r-bloggers.com/take-screenshot-of-webpage-using-r/", "rblogger.png")
webshot("https://www.r-bloggers.com/take-screenshot-of-webpage-using-r/", "rblogger.png", cliprect = "viewport")
```

# FFemeg 

- FFemeg is a collection of different project that handle multimedia files. The command line tool ffemeg(https://www.ffmpeg.org/ffmpeg.html) is the one I'm using to take screenshot of the video
- A very comprehensive tutorial: http://keycorner.org/pub/text/doc/ffmpeg-tutorial.htm

# Tutorial for ffepeg 

- Extracting images from video: ffmpeg -i inputfile.fmt -r 1 image%2d.jpg
  1) -r: frame rate: no. of frames per second: accept decimal
  2) image%2d.jpg: [name_of_image]_[numbering_system].jpg: 2d means 2 digits: 01, 02, 03... if 3d: 001, 002...
  3) ffmpeg -ss 00:03:00 -i nauru-a.mp4 -to 00:05:00 -c copy nauru-a_3to5.mp4


# youtube-dl - How does it work???

- "sorry, the video does not exist" 
- seems to use scripting to find the video and execute downloads

# Workflow

- use `youtube-dl` to download video from http://www.hcourt.gov.au/cases/recent-av-recordings: `youtube-dl [url_of_vidoe_page]` 
- use ffemeg to automatically extract frames for every minute from the video: `ffmpeg -i nauru-a.mp4 -r 1/60 image%2d.png` 

## OpenFace

- Baltrušaitis,	Tadas,	Peter	Robinson,	and	Louis-Philippe	Morency.	2016.	“Openface:	An	Open	Source	Facial	Behavior	Analysis	Toolkit.”	In	Applications	of	Computer	Vision	(WACV),	2016	IEEE	Winter	Conference	on,	1–10.	IEEE.
- https://www.cl.cam.ac.uk/research/rainbow/projects/openface/wacv2016.pdf
- see Mendelay

## OpenFace via Command line

- Has executable for sequence analysis that contains one face: `FeatureExtraction`; `FaceLandmarkVidMulti` is for sequence analysis that contains multiple faces; `FaceLandmarkImg`: for individual images that contains one or more faces 

## Meeting #4
- maybe try docker: since we have version issue 
- go get Taipan and fix stuff [tick]
- send to google/ OpenFace API to get desired data format

# Taipan

- We use Taipan to get the x and y coordinates of the face for each set of frame photos. Currently there are some issues with Shiny in terms of the dimension of the photos (can't select some part of the picture). We modify the app locally and manage to fix it - but not on the public version. 
- The output is stored in `taipan-export-2019-03-31.csv`

# Task week 4
- use `imager` or other tools to crop the faces from video frames 
- try docker version to use OpenFace to do face landmark for the cropped faces: http://cmusatyalab.github.io/openface/
- try google if OpenFace doesn't work 

# Crop faces from frames
The xy cordinate of the ideal faces is in taipan-export-2019-03-31. We can use command line tool magick to crop the whole image based on the cordinate supplied. The mogrify option allows for batch processing [may need to "mappify" the code]
```{r eval = FALSE}

xy_cord
cord <- xy_cord %>% 
  mutate(geom = paste0((xmax - xmin), "x",(ymax- ymin) , "+", xmin, "+", ymin))

nauru <- image_read("data/nauru-a/nauru-a_003.png")
image <- magick::image_crop(nauru, cord$geom)

nauru <- image_read(system.file())

```

```{r nauru-a, eval = FALSE}

cmd = c()
for (j in seq(1,3,1)){
  cmd[j] = paste0("cd data/nauru-a; magick mogrify -crop ", cord$geom[j + 25], " -path ../cropped/nauru-a", "/", j, " *.png")
}

for (i in 1:length(cmd)){
  system(cmd[i])
}
```

```{r nauru-b & 18-11-14a &18-11-14b, eval = FALSE}
# nauru-b
cmd = c()
for (j in seq(1,3,1)){
  cmd[j] = paste0("cd data/nauru-b; magick mogrify -crop ", cord$geom[j + 28], " -path ../cropped/nauru-b", "/", j, " *.png")
}

for (i in 1:length(cmd)){
  system(cmd[i])
}

#11-18-14a
cmd = c()
for (j in seq(1,5,1)){
  cmd[j] = paste0("cd data/18-11-14a; magick mogrify -crop ", cord$geom[j], " -path ../cropped/18-11-14a", "/", j, " *.png")
}

for (i in 1:length(cmd)){
  system(cmd[i])
}

#18-11-14b
cmd = c()
for (j in seq(1,5,1)){
  cmd[j] = paste0("cd data/18-11-14b; magick mogrify -crop ", cord$geom[j + 10], " -path ../cropped/18-11-14b", "/", j, " *.png")
}

for (i in 1:length(cmd)){
  system(cmd[i])
}
```

```{r 18-11-13&18-12-07&19-02-14, eval = FALSE}
cmd = c()
for (j in seq(1,5,1)){
  cmd[j] = paste0("cd data/18-11-13; magick mogrify -crop ", cord$geom[j + 5], " -path ../cropped/18-11-13", "/", j, " *.png")
}

for (i in 1:length(cmd)){
  system(cmd[i])
}

#18-12-07
cmd = c()
for (j in seq(1,5,1)){
  cmd[j] = paste0("cd data/18-12-07; magick mogrify -crop ", cord$geom[j + 15], " -path ../cropped/18-12-07", "/", j, " *.png")
}

for (i in 1:length(cmd)){
  system(cmd[i])
}

#19-02-14
cmd = c()
for (j in seq(1,5,1)){
  cmd[j] = paste0("cd data/19-02-14; magick mogrify -crop ", cord$geom[j + 20], " -path ../cropped/19-02-14", "/", j, " *.png")
}

for (i in 1:length(cmd)){
  system(cmd[i])
}
```

```{r eval = FALSE}

for (i in nauru){
  for (j in seq(26, 31, 1)){
    cmd[i] = paste0("cd data/", i, "; magick mogrify -crop ", cord$geom[j], " -path ../cropped/", i, " *.png")
  }
}

system(cmd)
which("nauru-a" == substr(cord$image_name, 1,7))

j = seq(26, 31, 1)
```


# Docker for OpenFace API

- citation: OpenFace 2.0: Facial Behavior Analysis Toolkit Tadas Baltrušaitis, Amir Zadeh, Yao Chong Lim, and Louis-Philippe Morency, IEEE International Conference on Automatic Face and Gesture Recognition, 2018
- a good starting point: https://docker-curriculum.com/
- Command used: 
  1.  docker cp cropped/nauru-a/1 b811e9b14812:/home/openface-build 
  2.  build/bin/FaceLandmarkImg -f nauru-a_003.png
  3.  build/bin/FaceLandmarkImg -fdir data/1/
  4.  docker cp b811e9b14812:/home/openface-build/processed/nauru-a_003.csv ~
- First pull the "repo" from DockerHub and run it via docker run -it --rm algebr/openface:latest. This opens a docker terminal. Copy the directory from local to docker outside the docker using 1). Use 2) inside the docker for single image processing adn 3) for batch processing. Use 4) to copy the processed file from docker to local machine. 


Meeting #5
- add image id 
- readr::read_csv() - batch process .csv's

