---
title: "Collecting data"
author: "Huize Zhang"
date: "13/03/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(jsonlite)
library(purrr)
library(tidyverse)
library(rvest)
```

# Documentation
## OpenFace 
- OpenFace(https://cmusatyalab.github.io/openface/) is a broad application to detect face  
- notice that there are two version of it in github: "cmusatyalab/openface" and "TadasBaltrusaitis/OpenFace". 
- There is also a docker version  https://hub.docker.com/r/bamos/openface/

## OpenCV
- OpenCV(https://opencv.org/opencv-4-0-0.html) looks to be a good tool to crop face from image while a lot of the example/ tutorials are in C++ or python
- It has its own version in R called `ROpenCVLite`. I manage to get it installed: "swarm-lab/ROpenCVLite", but have no idea how to use it - haven't found tutorial/ documentation on that: https://swarm-lab.github.io/ROpenCVLite/

## OpenPose
- CMU-Perceptual-Computing-Lab/openpose
- https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/standalone_face_or_hand_keypoint_detector.md

## video/ image
- google can't really recognise face if it is the case of 7 judges because faces are too small and the video is only 720p
- I manage to get the data from the faces that google recognised in a desiable format - see the chunk below
- find that usually judges are looking down because the lawyer will guide them to look at certain pages of the book. Only when one or more of the judge is asking a question, most of the judge will look up to the camera. 


## Further for week 3
using r script to take screenshot of the media 
- ffemeg(looks doable):https://www.ffmpeg.org/ffprobe.html 
- PhantomJS
- R: webshot, takescreenshot
- have a look at how to do openpose 

# Google API
```{r}

dt <- jsonlite::fromJSON("goog_oks.json")

landmark <- dt$faceAnnotations$landmarks
names(landmark) <- paste0("face_",rep(1:length(landmark)))

landmark2 <- list(landmark[["face_1"]][["position"]],
                  landmark[["face_2"]][["position"]],
                  landmark[["face_3"]][["position"]])

face <- data.frame(matrix(unlist(landmark2), nrow=length(landmark2), 
                          ncol =nrow(landmark2[[1]]),byrow=T))

names(face) = landmark[[1]]$type
face_oks <- face %>% mutate(ID = seq(1:nrow(face)))

```

# Scraping the list of video
```{r}
url = "http://www.hcourt.gov.au/cases/recent-av-recordings"
webpage = read_html(url)
nodes<- html_nodes(webpage, xpath = '//*[@id="container"]/div[3]/div[1]/table')
tbody <- html_table(nodes)
recording <- data.frame(tbody)
names(recording) <- recording[1,]
recording <- recording[-c(1,nrow(recording)),-c(4)]
```

```{r}
library(ROpenCVLite) #devtools::install_github("swarm-lab/ROpenCVLite")
library(Rvision)

```

## week 3 Task
using r script to take screenshot of the media 
- ffemeg(looks doable):https://www.ffmpeg.org/ffprobe.html 
- PhantomJS
- R: webshot, takescreenshot

# webshot() and takescreenshot() from R
`webshot()` can take a screenshot of the website but only for static website. Don't think can do it for videos

```{r eval = FALSE}
library(webshot)
webshot("https://www.r-project.org/", "r.png")
webshot("https://www.r-bloggers.com/take-screenshot-of-webpage-using-r/", "rblogger.png")
webshot("https://www.r-bloggers.com/take-screenshot-of-webpage-using-r/", "rblogger.png", cliprect = "viewport")
```

# FFemeg 

- FFemeg is a collection of different project that handle multimedia files. The command line tool ffemeg(https://www.ffmpeg.org/ffmpeg.html) is the one I'm using to take screenshot of the video
- A very comprehensive tutorial: http://keycorner.org/pub/text/doc/ffmpeg-tutorial.htm

# Tutorial for ffepeg 

- Extracting images from video: ffmpeg -i inputfile.fmt -r 1 image%2d.jpg
  1) -r: frame rate: no. of frames per second: accept decimal
  2) image%2d.jpg: [name_of_image]_[numbering_system].jpg: 2d means 2 digits: 01, 02, 03... if 3d: 001, 002...

# youtube-dl - How does it work???

- "sorry, the video does not exist" 
- seems to use scripting to find the video and execute downloads

# Workflow

- use `youtube-dl` to download video from http://www.hcourt.gov.au/cases/recent-av-recordings: `youtube-dl [url_of_vidoe_page]` 
- use ffemeg to automatically extract frames for every minute from the video: `ffmpeg -i inputfile.fmt -r 1 image%2d.jpg` 

## OpenFace

- Baltrušaitis,	Tadas,	Peter	Robinson,	and	Louis-Philippe	Morency.	2016.	“Openface:	An	Open	Source	Facial	Behavior	Analysis	Toolkit.”	In	Applications	of	Computer	Vision	(WACV),	2016	IEEE	Winter	Conference	on,	1–10.	IEEE.
- https://www.cl.cam.ac.uk/research/rainbow/projects/openface/wacv2016.pdf
- see Mendelay

## OpenFace via Command line

- Has executable for sequence analysis that contains one face: `FeatureExtraction`; `FaceLandmarkVidMulti` is for sequence analysis that contains multiple faces; `FaceLandmarkImg`: for individual images that contains one or more faces 

## meeting #4
- maybe try docker: since we have version issue 
- go get Taipan and fix stuff [tick]
- send to google/ OpenFace API to get desired data format

# Taipan

- We use Taipan to get the x and y coordinates of the face for each set of frame photos. Currently there are some issues with Shiny in terms of the dimension of the photos (can't select some part of the picture). We modify the app locally and manage to fix it - but not on the public version. 
- The output is stored in `taipan-export.csv`

# Task week 4
- use `imager` or other tools to crop the faces from video frames 
- try docker version to use OpenFace to do face landmark for the cropped faces: http://cmusatyalab.github.io/openface/
- try google if OpenFace doesn't work 
